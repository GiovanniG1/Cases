{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced_Programming10_SuperCase_1_Structured_data_v4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puozYNWhp85y"
      },
      "source": [
        "## Confidentiality\r\n",
        "\r\n",
        "The programmatic cases in this notebook are utilized from different internet resources (in this notebook especially from kaggle.com) and are for demonstrational purposes only.\r\n",
        "\r\n",
        "Please do not copy or distribute this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYh2E9iep-Md"
      },
      "source": [
        "## Table of content\r\n",
        "\r\n",
        "Census Income Data\r\n",
        "\r\n",
        "1. Programmatic case 1 \r\n",
        "2. Programmatic case 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNNVI_kHqBnY"
      },
      "source": [
        "## Previous knowledge\r\n",
        "\r\n",
        "For a good understanding of this notebook you should have a few years of data-science and programming experience and have studied the advanced programming notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYv_Jyw6qvrx"
      },
      "source": [
        "## Introduction\r\n",
        "\r\n",
        "A supercase is a case for a dataset on which multiple data-science methods and techniques can be applied.\r\n",
        "There is no predifined goal. The goal is to explore cases for the dataset with multiple data-science methods, techniques and programs.\r\n",
        "The goal is to built more specific cases with specific goals. A supercase contains information to built multiple new specific cases. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhhWdLl0qBpx"
      },
      "source": [
        "#### Programmatic case 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9CH-6oiuX4U"
      },
      "source": [
        "######################################################################################################################\r\n",
        "######################################################################################################################\r\n",
        "##2) Prediction 1 - Advanced Program \r\n",
        "\r\n",
        "#2.1) LGBM Ensemble"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moxjZebleJYN"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from lightgbm import LGBMClassifier\r\n",
        "\r\n",
        "!pip install catboost\r\n",
        "!pip install ipywidgets\r\n",
        "!jupyter nbextension enable --py widgetsnbextension\r\n",
        "from catboost import CatBoostClassifier\r\n",
        "\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "# Regularized Greedy Forest\r\n",
        "!pip install rgf_python -v\r\n",
        "from rgf.sklearn import RGFClassifier     # https://github.com/fukatani/rgf_python\r\n",
        "\r\n",
        "import datetime\r\n",
        "import pandas as pd\r\n",
        "import xgboost as xgb\r\n",
        "import numpy as np\r\n",
        "import json, math, os, sys\r\n",
        "\r\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.pipeline import FeatureUnion, make_pipeline\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\r\n",
        "\r\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\r\n",
        "\r\n",
        "\r\n",
        "census_data_filename = './adult.data'\r\n",
        "\r\n",
        "# These are the column labels from the census data files\r\n",
        "COLUMNS = (\r\n",
        "    'age',\r\n",
        "    'workclass',\r\n",
        "    'fnlwgt',\r\n",
        "    'education',\r\n",
        "    'education-num',\r\n",
        "    'marital-status',\r\n",
        "    'occupation',\r\n",
        "    'relationship',\r\n",
        "    'Clothing',\r\n",
        "    'Gender',\r\n",
        "    'capital-gain',\r\n",
        "    'capital-loss',\r\n",
        "    'hours-per-week',\r\n",
        "    'native-country',\r\n",
        "    'income-level'\r\n",
        ")\r\n",
        "\r\n",
        "# Loading the training census dataset\r\n",
        "with open(census_data_filename, 'r') as train_data:\r\n",
        "    raw_training_data = pd.read_csv(train_data, header=None, names=COLUMNS)\r\n",
        "\r\n",
        "# Class 1: Pre-processing\r\n",
        "raw_training_data.head()\r\n",
        "\r\n",
        "train = raw_training_data.drop('income-level', axis=1).values\r\n",
        "# Creating training labels list\r\n",
        "target_train = (raw_training_data['income-level'] == ' >50K').values\r\n",
        "\r\n",
        "target_train, bin= train_test_split(target_train, train_size=0.70)\r\n",
        "\r\n",
        "\r\n",
        "# Class 1.1 Positionalselector\r\n",
        "class PositionalSelector(BaseEstimator, TransformerMixin):\r\n",
        "    def __init__(self, positions):\r\n",
        "        self.positions = positions\r\n",
        "\r\n",
        "    def fit(self, X, y=None):\r\n",
        "        return self\r\n",
        "\r\n",
        "    def transform(self, X):\r\n",
        "        return np.array(X)[:, self.positions]\r\n",
        "\r\n",
        "# Class 1.2 CleanString\r\n",
        "class CleanString(BaseEstimator, TransformerMixin):\r\n",
        "    def fit(self, X, y=None):\r\n",
        "        return self\r\n",
        "\r\n",
        "    def transform(self, X):\r\n",
        "        strip = np.vectorize(str.strip)\r\n",
        "        return strip(np.array(X))\r\n",
        "\r\n",
        "# Class 1.3 SimpleOneHotEncoder\r\n",
        "class SimpleOneHotEncoder(BaseEstimator, TransformerMixin):\r\n",
        "    def fit(self, X, y=None):\r\n",
        "        self.values = []\r\n",
        "        for c in range(X.shape[1]):\r\n",
        "            Y = X[:, c]\r\n",
        "            values = {v: i for i, v in enumerate(np.unique(Y))}\r\n",
        "            self.values.append(values)\r\n",
        "        return self\r\n",
        "\r\n",
        "    def transform(self, X):\r\n",
        "        X = np.array(X)\r\n",
        "        matrices = []\r\n",
        "        for c in range(X.shape[1]):\r\n",
        "            Y = X[:, c]\r\n",
        "            matrix = np.zeros(shape=(len(Y), len(self.values[c])), dtype=np.int8)\r\n",
        "            for i, x in enumerate(Y):\r\n",
        "                if x in self.values[c]:\r\n",
        "                    matrix[i][self.values[c][x]] = 1\r\n",
        "            matrices.append(matrix)\r\n",
        "        res = np.concatenate(matrices, axis=1)\r\n",
        "        return res\r\n",
        "\r\n",
        "# Class 1.4 Makepipeline\r\n",
        "\r\n",
        "# Categorical features: age and hours-per-week\r\n",
        "# Numerical features: workclass, marital-status, and relationship\r\n",
        "numerical_indices = [0, 12]  # age-num, and hours-per-week\r\n",
        "categorical_indices = [1, 3, 5, 7]  # workclass, education, marital-status, and relationship\r\n",
        "\r\n",
        "p1 = make_pipeline(PositionalSelector(categorical_indices), \r\n",
        "                   StripString(), \r\n",
        "                   SimpleOneHotEncoder())\r\n",
        "p2 = make_pipeline(PositionalSelector(numerical_indices), \r\n",
        "                   StandardScaler())\r\n",
        "\r\n",
        "pipeline = FeatureUnion([\r\n",
        "    ('numericals', p1),\r\n",
        "    ('categoricals', p2),\r\n",
        "])\r\n",
        "\r\n",
        "train_features = pipeline.fit_transform(raw_features)\r\n",
        "\r\n",
        "train, test= train_test_split(train_features, train_size=0.70)\r\n",
        "print(train.shape, test.shape)\r\n",
        "\r\n",
        "\r\n",
        "# Class 2: Ensembling\r\n",
        "\r\n",
        "# Class 2.1 Ensemble\r\n",
        "class Ensemble(object):\r\n",
        "    def __init__(self, n_splits, stacker, base_models):\r\n",
        "        self.n_splits = n_splits\r\n",
        "        self.stacker = stacker\r\n",
        "        self.base_models = base_models\r\n",
        "\r\n",
        "    def fit_predict(self, X, y, T):\r\n",
        "        X = np.array(X)\r\n",
        "        y = np.array(y)\r\n",
        "        T = np.array(T)\r\n",
        "\r\n",
        "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=2016).split(X, y))\r\n",
        "\r\n",
        "        S_train = np.zeros((X.shape[0], len(self.base_models)))\r\n",
        "        S_test = np.zeros((T.shape[0], len(self.base_models)))\r\n",
        "        for i, clf in enumerate(self.base_models):\r\n",
        "\r\n",
        "            S_test_i = np.zeros((T.shape[0], self.n_splits))\r\n",
        "\r\n",
        "            for j, (train_idx, test_idx) in enumerate(folds):\r\n",
        "                X_train = X[train_idx]\r\n",
        "                y_train = y[train_idx]\r\n",
        "                X_holdout = X[test_idx]\r\n",
        "                # y_holdout = y[test_idx]\r\n",
        "\r\n",
        "                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\r\n",
        "                clf.fit(X_train, y_train)\r\n",
        "                # cross_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc')\r\n",
        "                # print(\"    cross_score: %.5f\" % (cross_score.mean()))\r\n",
        "                y_pred = clf.predict_proba(X_holdout)[:,1]                \r\n",
        "\r\n",
        "                S_train[test_idx, i] = y_pred\r\n",
        "                S_test_i[:, j] = clf.predict_proba(T)[:,1]\r\n",
        "            S_test[:, i] = S_test_i.mean(axis=1)\r\n",
        "\r\n",
        "        results = cross_val_score(self.stacker, S_train, y, cv=3, scoring='roc_auc')\r\n",
        "        print(\"Stacker score: %.5f\" % (results.mean()))\r\n",
        "\r\n",
        "        self.stacker.fit(S_train, y)\r\n",
        "        res = self.stacker.predict_proba(S_test)[:,1]\r\n",
        "        return res\r\n",
        "\r\n",
        "# Class 4.2 Parameters\r\n",
        "# LightGBM params\r\n",
        "lgb_params = {}\r\n",
        "lgb_params['learning_rate'] = 0.02\r\n",
        "lgb_params['n_estimators'] = 650\r\n",
        "lgb_params['max_bin'] = 10\r\n",
        "lgb_params['subsample'] = 0.8\r\n",
        "lgb_params['subsample_freq'] = 10\r\n",
        "lgb_params['colsample_bytree'] = 0.8   \r\n",
        "lgb_params['min_child_samples'] = 500\r\n",
        "lgb_params['seed'] = 99\r\n",
        "\r\n",
        "lgb_params2 = {}\r\n",
        "lgb_params2['n_estimators'] = 1090\r\n",
        "lgb_params2['learning_rate'] = 0.02\r\n",
        "lgb_params2['colsample_bytree'] = 0.3   \r\n",
        "lgb_params2['subsample'] = 0.7\r\n",
        "lgb_params2['subsample_freq'] = 2\r\n",
        "lgb_params2['num_leaves'] = 16\r\n",
        "lgb_params2['seed'] = 99\r\n",
        "\r\n",
        "lgb_params3 = {}\r\n",
        "lgb_params3['n_estimators'] = 1100\r\n",
        "lgb_params3['max_depth'] = 4\r\n",
        "lgb_params3['learning_rate'] = 0.02\r\n",
        "lgb_params3['seed'] = 99\r\n",
        "\r\n",
        "\r\n",
        "lgb_model = LGBMClassifier(**lgb_params)\r\n",
        "\r\n",
        "lgb_model2 = LGBMClassifier(**lgb_params2)\r\n",
        "\r\n",
        "lgb_model3 = LGBMClassifier(**lgb_params3)\r\n",
        "\r\n",
        "\r\n",
        "log_model = LogisticRegression()\r\n",
        "\r\n",
        "        \r\n",
        "stack = Ensemble(n_splits=3,\r\n",
        "        stacker = log_model,\r\n",
        "        base_models = (lgb_model, lgb_model2, lgb_model3))        \r\n",
        "        \r\n",
        "y_pred = stack.fit_predict(train, target_train, test)        \r\n",
        "\r\n",
        "\r\n",
        "sub = pd.DataFrame()\r\n",
        "sub['target'] = y_pred\r\n",
        "sub.to_csv('stacked_1.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0c47VV__CEK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHDe-R5h_CR5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi94q7Ej_GXu"
      },
      "source": [
        "######################################################################################################################\r\n",
        "######################################################################################################################\r\n",
        "##2) Prediction 2 - Advanced Program \r\n",
        "\r\n",
        "#2.2) Tensorflow linear classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfrnu2GWgMrg"
      },
      "source": [
        "import math\r\n",
        "\r\n",
        "from IPython import display\r\n",
        "from matplotlib import cm\r\n",
        "from matplotlib import gridspec\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from mpl_toolkits.mplot3d import Axes3D\r\n",
        "from sklearn import metrics\r\n",
        "\r\n",
        "!pip install tensorflow\r\n",
        "!pip install tensorflow_gpu\r\n",
        "!pip install \"tensorflow==1.14.0\"\r\n",
        "!pip install \"tensorflow_gpu==1.14.0\"\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.contrib.learn.python.learn import learn_io, estimator\r\n",
        "\r\n",
        "# Increasing the amount of logging when there is an error\r\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\r\n",
        "\r\n",
        "# Setting the output display to have two digits for decimal places, for display\r\n",
        "# readability only and limit it to printing 10 rows.\r\n",
        "pd.options.display.float_format = '{:.2f}'.format\r\n",
        "pd.options.display.max_rows = 20\r\n",
        "\r\n",
        "import tempfile\r\n",
        "import urllib\r\n",
        "train_file = tempfile.NamedTemporaryFile()\r\n",
        "urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\", train_file.name)\r\n",
        "\r\n",
        "COLUMNS = [\"age\", \"workclass\", \"sample_weight\", \"education\", \"education_num\",\r\n",
        "           \"marital_status\", \"occupation\", \"relationship\", \"clothing\", \"gender\",\r\n",
        "           \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\",\r\n",
        "           \"income_bracket\"]\r\n",
        "census_df = pd.read_csv(train_file, names=COLUMNS, skipinitialspace=True)\r\n",
        "# Presenting first 5 rows of the table.\r\n",
        "census_df.head(5)\r\n",
        "\r\n",
        "\r\n",
        "CATEGORICAL_COLUMNS = [\"workclass\", \"education\", \"marital_status\", \"occupation\",\r\n",
        "                       \"relationship\", \"clothing\", \"gender\", \"native_country\"]\r\n",
        "NUMERICAL_COLUMNS = [\"age\", \"education_num\", \"capital_gain\", \"capital_loss\",\r\n",
        "                      \"hours_per_week\"]\r\n",
        "LABEL = \"income_over_50k\"\r\n",
        "\r\n",
        "def input_fn(dataframe):\r\n",
        "  \"\"\"Constructs a dictionary for the feature columns.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    dataframe: The Pandas DataFrame to use for the input.\r\n",
        "  Returns:\r\n",
        "    The feature columns and the associated labels for the provided input.\r\n",
        "  \"\"\"\r\n",
        "  # Creating a dictionary mapping each numerical feature column name (k) to\r\n",
        "  # the values of that column stored in a constant Tensor.\r\n",
        "  numerical_cols = {k: tf.constant(dataframe[k].values) \r\n",
        "                    for k in NUMERICAL_COLUMNS}\r\n",
        "  # Creating a dictionary mapping each categorical feature column name (k)\r\n",
        "  # to the values of that column stored in a tf.SparseTensor.\r\n",
        "  categorical_cols = {k: tf.SparseTensor(\r\n",
        "      indices=[[i, 0] for i in range(dataframe[k].size)],\r\n",
        "      values=dataframe[k].values,\r\n",
        "      dense_shape=[dataframe[k].size, 1])\r\n",
        "                      for k in CATEGORICAL_COLUMNS}\r\n",
        "  # Merging the two dictionaries into one.\r\n",
        "  feature_cols = dict(numerical_cols.items() + categorical_cols.items())\r\n",
        "  # Converting the label column into a constant Tensor.\r\n",
        "  label = tf.constant(dataframe[LABEL].values)\r\n",
        "  # Returning the feature columns and the label.\r\n",
        "  return feature_cols, label\r\n",
        "\r\n",
        "def train_input_fn():\r\n",
        "  return input_fn(training_examples)\r\n",
        "\r\n",
        "def eval_input_fn():\r\n",
        "  return input_fn(validation_examples)\r\n",
        "\r\n",
        "def test_input_fn():\r\n",
        "  return input_fn(test_examples)\r\n",
        "\r\n",
        "\r\n",
        "# Linearly rescaling to the range [0, 1]\r\n",
        "def linear_scale(series):\r\n",
        "  min_val = series.min()\r\n",
        "  max_val = series.max()\r\n",
        "  scale = 1.0 * (max_val - min_val)\r\n",
        "  return series.apply(lambda x:((x - min_val) / scale))\r\n",
        "\r\n",
        "def prepare_features(dataframe):\r\n",
        "  \"\"\"Prepares the features for provided dataset.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    dataframe: A Pandas DataFrame expected to contain the data.\r\n",
        "  Returns:\r\n",
        "    A new DataFrame that contains the features to be used for the model.\r\n",
        "  \"\"\"\r\n",
        "  processed_features = dataframe.copy()\r\n",
        "  for feature in NUMERICAL_COLUMNS:\r\n",
        "    processed_features[feature] = linear_scale(dataframe[feature])\r\n",
        "    \r\n",
        "  # Converting the output target to 0 (for <=50k) and 1 (> 50k)\r\n",
        "  processed_features[LABEL] = dataframe[\"income_bracket\"].apply(\r\n",
        "      lambda x: \">50K\" in x).astype(int)\r\n",
        "  \r\n",
        "  return processed_features\r\n",
        "\r\n",
        "\r\n",
        "census_df = census_df.reindex(np.random.permutation(census_df.index))\r\n",
        "training_examples = prepare_features(census_df.head(12281))\r\n",
        "validation_examples = prepare_features(census_df.tail(4000))\r\n",
        "\r\n",
        "test_file = tempfile.NamedTemporaryFile()\r\n",
        "urllib.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\", test_file.name)\r\n",
        "\r\n",
        "census_df_test = pd.read_csv(test_file, names=COLUMNS, skipinitialspace=True, skiprows=1)\r\n",
        "test_examples = prepare_features(census_df_test)\r\n",
        "\r\n",
        "\r\n",
        "def make_roc_curve(predictions, targets):\r\n",
        "  \"\"\" Plots an ROC curve for the provided predictions and targets.\r\n",
        "\r\n",
        "  Args:\r\n",
        "    predictions: the probability that the example has label 1.\r\n",
        "    targets: a list of the target values being predicted that must be the\r\n",
        "             same length as predictions.\r\n",
        "  \"\"\"  \r\n",
        "  false_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(\r\n",
        "      targets, predictions)\r\n",
        "  \r\n",
        "  plt.ylabel(\"true positive rate\")\r\n",
        "  plt.xlabel(\"false positive rate\")\r\n",
        "  plt.plot(false_positive_rate, true_positive_rate)\r\n",
        "\r\n",
        "  \r\n",
        "def plot_learning_curve(training_losses, validation_losses):\r\n",
        "  \"\"\" Plot the learning curve.\r\n",
        "  \r\n",
        "  Args:\r\n",
        "    training_loses: a list of training losses to plot.\r\n",
        "    validation_losses: a list of validation losses to plot.\r\n",
        "  \"\"\"        \r\n",
        "  plt.ylabel('Loss')\r\n",
        "  plt.xlabel('Training Steps')\r\n",
        "  plt.plot(training_losses, label=\"training\")\r\n",
        "  plt.plot(validation_losses, label=\"validation\")\r\n",
        "  plt.legend(loc=1)\r\n",
        "\r\n",
        "\r\n",
        "def compute_loss(model, input_fn, targets):\r\n",
        "  \"\"\" Computes the log loss for training a linear classifier.\r\n",
        "  \r\n",
        "  Args:\r\n",
        "    model: the trained model to use for making the predictions.\r\n",
        "    input_fn: the input_fn to use to make the predicitons.\r\n",
        "    targets: a list of the target values being predicted that must be the\r\n",
        "             same length as predictions.\r\n",
        "    \r\n",
        "  Returns:\r\n",
        "    The log loss for the provided predictions and targets.\r\n",
        "  \"\"\"      \r\n",
        "  \r\n",
        "  predictions = np.array(list(model.predict_proba(input_fn=input_fn)))\r\n",
        "  return metrics.log_loss(targets, predictions[:, 1])\r\n",
        "\r\n",
        "\r\n",
        "def construct_feature_columns():\r\n",
        "  \"\"\"Construct TensorFlow Feature Columns for features\r\n",
        "  \r\n",
        "  Returns:\r\n",
        "    A set of feature columns.\r\n",
        "  \"\"\"\r\n",
        "  \r\n",
        "  # Sample creating a categorical column with known values.\r\n",
        "  gender = tf.contrib.layers.sparse_column_with_keys(\r\n",
        "    column_name=\"gender\", keys=[\"Female\", \"Male\"])\r\n",
        "  \r\n",
        "  # Sample creating a categorical columns with a hash bucket.    \r\n",
        "  education = tf.contrib.layers.sparse_column_with_hash_bucket(\r\n",
        "      \"education\", hash_bucket_size=50)\r\n",
        "  \r\n",
        "  # Sample creating a real-valued column for numeric data.\r\n",
        "  age = tf.contrib.layers.real_valued_column(\"age\") \r\n",
        "\r\n",
        "  # Returning the set of all feature columns generated.\r\n",
        "  feature_columns=[gender, education, age]\r\n",
        " \r\n",
        "  return feature_columns\r\n",
        "\r\n",
        "\r\n",
        "def define_linear_classifier(learning_rate):\r\n",
        "  \"\"\" Defines a linear classifer to predict the target.\r\n",
        "  \r\n",
        "  Args:\r\n",
        "    learning_rate: A `float`, the learning rate.\r\n",
        "    \r\n",
        "  Returns:\r\n",
        "    A linear classifier created with the given parameters.\r\n",
        "  \"\"\"\r\n",
        "  linear_classifier = tf.contrib.learn.LinearClassifier(\r\n",
        "    feature_columns=construct_feature_columns(),\r\n",
        "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate),\r\n",
        "    gradient_clip_norm=5.0\r\n",
        "  )  \r\n",
        "  return linear_classifier\r\n",
        "\r\n",
        "\r\n",
        "def train_model(model, steps):\r\n",
        "  \"\"\"Trains a linear classifier.\r\n",
        "  \r\n",
        "  Args:\r\n",
        "    model: The model to train.\r\n",
        "    steps: A non-zero `int`, the total number of training steps.\r\n",
        "    \r\n",
        "  Returns:\r\n",
        "    The trained model.\r\n",
        "  \"\"\"\r\n",
        "  # In order to see how the model evolves as it is trained, the steps\r\n",
        "  # are divided into periods and presents the model after each period.\r\n",
        "  periods = 10\r\n",
        "  steps_per_period = steps / periods\r\n",
        "  \r\n",
        "  # Training the model, but doing so inside a loop so that you can periodically assess\r\n",
        "  # loss metrics.  Storing the training and validation losses so you can\r\n",
        "  # generate a learning curve.\r\n",
        "  print \"Training model...\"\r\n",
        "  training_losses = []\r\n",
        "  validation_losses = []\r\n",
        "\r\n",
        "  for period in range (0, periods):\r\n",
        "    # Calling fit to train the model for steps_per_period steps.\r\n",
        "    model.fit(input_fn=train_input_fn, steps=steps_per_period)\r\n",
        "    \r\n",
        "    # Computing the loss between the predictions and the correct labels, appending\r\n",
        "    # the training and validation loss to the list of losses used to generate\r\n",
        "    # the learning curve after training is complete and printing the current\r\n",
        "    # training loss.\r\n",
        "    training_loss = compute_loss(model, train_input_fn,\r\n",
        "                                 training_examples[LABEL])\r\n",
        "    validation_loss = compute_loss(model, eval_input_fn,\r\n",
        "                                   validation_examples[LABEL])\r\n",
        "    training_losses.append(training_loss) \r\n",
        "    validation_losses.append(validation_loss) \r\n",
        "    print \"  Training loss after period %02d : %0.3f\" % (period, training_loss)\r\n",
        "      \r\n",
        "  # Now that training is done, printing the final training and validation losses.  \r\n",
        "  print \"Final Training Loss: %0.3f\" % training_loss\r\n",
        "  print \"Final Validation Loss: %0.3f\" % validation_loss \r\n",
        "  \r\n",
        "  # Generating a figure with the learning curve on the left and an ROC curve on\r\n",
        "  # the right.\r\n",
        "  plt.figure(figsize=(10, 5))\r\n",
        "  plt.subplot(1, 2, 1)\r\n",
        "  plt.title(\"Learning Curve (Loss vs time)\")\r\n",
        "  plot_learning_curve(training_losses, validation_losses)\r\n",
        "  \r\n",
        "  plt.subplot(1, 2, 2)\r\n",
        "  plt.tight_layout(pad=1.1, w_pad=3.0, h_pad=3.0) \r\n",
        "  plt.title(\"ROC Curve on Validation Data\")\r\n",
        "  validation_probabilities = np.array(list(model.predict_proba(\r\n",
        "    input_fn=eval_input_fn)))\r\n",
        "  # ROC curve uses the probability that the label is 1.\r\n",
        "  make_roc_curve(validation_probabilities[:, 1], validation_examples[LABEL])\r\n",
        "   \r\n",
        "  return model\r\n",
        "\r\n",
        "\r\n",
        "LEARNING_RATE = 0.1\r\n",
        "STEPS = 250\r\n",
        "\r\n",
        "linear_classifier = define_linear_classifier(learning_rate = LEARNING_RATE)\r\n",
        "linear_classifier = train_model(linear_classifier, steps=STEPS)\r\n",
        "\r\n",
        "\r\n",
        "evaluation_metrics = linear_classifier.evaluate(\r\n",
        " input_fn=eval_input_fn, steps=1)\r\n",
        "\r\n",
        "print \"AUC on the validation set: %0.2f\" % evaluation_metrics['auc']\r\n",
        "print \"Accuracy on the validation set: %0.2f\" % evaluation_metrics['accuracy']\r\n",
        "print \"Loss on the validation set: %0.2f\" % evaluation_metrics['loss']\r\n",
        "\r\n",
        "\r\n",
        "def construct_feature_columns():\r\n",
        "  \"\"\"Construct TensorFlow Feature Columns for features\r\n",
        "  \r\n",
        "  Returns:\r\n",
        "    A set of feature columns.\r\n",
        "  \"\"\"\r\n",
        "  \r\n",
        "  # Sample creating a categorical column with known values.\r\n",
        "  gender = tf.contrib.layers.sparse_column_with_keys(\r\n",
        "    column_name=\"gender\", keys=[\"Female\", \"Male\"])\r\n",
        "  \r\n",
        "  # Sample creating a categorical columns with a hash bucket.    \r\n",
        "  education = tf.contrib.layers.sparse_column_with_hash_bucket(\r\n",
        "      \"education\", hash_bucket_size=50)\r\n",
        "  \r\n",
        "  # Sample creating a real-valued column that can be used for numerical data.\r\n",
        "  age = tf.contrib.layers.real_valued_column(\"age\") \r\n",
        "\r\n",
        "  # Returning the set of all feature columns generated.\r\n",
        "  feature_columns=[gender, education, age]\r\n",
        " \r\n",
        "  return feature_columns\r\n",
        "\r\n",
        "LEARNING_RATE = 0.1\r\n",
        "STEPS = 100\r\n",
        "\r\n",
        "linear_classifier = define_linear_classifier(learning_rate = LEARNING_RATE)\r\n",
        "linear_classifier = train_model(linear_classifier, steps=STEPS)\r\n",
        "\r\n",
        "\r\n",
        "evaluation_metrics = linear_classifier.evaluate(\r\n",
        " input_fn=eval_input_fn, steps=1)\r\n",
        "\r\n",
        "print \"AUC on the validation set: %0.2f\" % evaluation_metrics['auc']\r\n",
        "print \"Accuracy on the validation set: %0.2f\" % evaluation_metrics['accuracy']\r\n",
        "print \"Loss on the validation set: %0.2f\" % evaluation_metrics['loss']"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}