{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced_Programming_11_SuperCase_3_Structured_data_v7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOxHxWmMDwCJ"
      },
      "source": [
        "## Confidentiality\r\n",
        "\r\n",
        "The programmatic cases in this notebook are utilized from different internet resources (in this notebook especially from kaggle.com) and are for demonstrational purposes only.\r\n",
        "\r\n",
        "Please do not copy or distribute this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PxBY8qNDzO-"
      },
      "source": [
        "## Table of content\r\n",
        "\r\n",
        "Census Income Data\r\n",
        "\r\n",
        "1. Programmatic case 1 \r\n",
        "2. Programmatic case 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqNcxE5ED4Gn"
      },
      "source": [
        "## Previous knowledge\r\n",
        "\r\n",
        "For a good understanding of this notebook you should have a few years of data-science and programming experience and have studied the advanced programming notebooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnwDu5fkD7Lu"
      },
      "source": [
        "## Introduction\r\n",
        "\r\n",
        "A supercase is a case for a dataset on which multiple data-science methods and techniques can be applied.\r\n",
        "There is no predifined goal. The goal is to explore cases for the dataset with multiple data-science methods, techniques and programs.\r\n",
        "The goal is to built more specific cases with specific goals. A supercase contains information to built multiple new specific cases. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KCmt4PPFCD6"
      },
      "source": [
        "Programmatic case 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqvijhSOFCEG"
      },
      "source": [
        "######################################################################################################################\r\n",
        "######################################################################################################################\r\n",
        "##1) Prediction 1 - Advanced Program \r\n",
        "\r\n",
        "#1.1) Tensorflow Boosted tree's"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWbZeKVdQIvq"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from IPython.display import clear_output\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "train_path = '/content/census_data.csv'\r\n",
        "\r\n",
        "train = pd.read_csv(train_path)\r\n",
        "\r\n",
        "train[\"income_level_cat2\"] = train[\"income_level_cat\"].astype('category')\r\n",
        "train.dtypes\r\n",
        "\r\n",
        "train[\"income_level_cat2\"] = train[\"income_level_cat2\"].cat.codes\r\n",
        "train.head()\r\n",
        "\r\n",
        "# split into train test sets\r\n",
        "X = train\r\n",
        "X_train, X_test= train_test_split(X, train_size=0.70)\r\n",
        "print(X_train.shape, X_test.shape)\r\n",
        "\r\n",
        "# split features and dependent\r\n",
        "y_train = X_train[\"income_level_cat2\"]\r\n",
        "dftrain = X_train.drop(\"income_level_cat2\", axis=1)\r\n",
        "y_eval = X_test[\"income_level_cat2\"]\r\n",
        "dfeval = X_test.drop(\"income_level_cat2\", axis=1)\r\n",
        "\r\n",
        "train.dtypes\r\n",
        "\r\n",
        "train = train.drop(\"income_level_cat\", axis=1)\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "tf.random.set_seed(123)\r\n",
        "\r\n",
        "dftrain.head()\r\n",
        "\r\n",
        "ftrain.describe()\r\n",
        "\r\n",
        "dftrain.shape[0], dfeval.shape[0]\r\n",
        "\r\n",
        "dftrain.Age.hist(bins=20)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "dftrain.Gender.value_counts().plot(kind='barh')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "dftrain['education-num'].value_counts().plot(kind='barh')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "dftrain['relationship'].value_counts().plot(kind='barh')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "pd.concat([dftrain, y_train], axis=1).groupby('Gender').income_level_cat2.mean().plot(kind='barh').set_xlabel('% income_level_cat2')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "CATEGORICAL_COLUMNS = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation',\r\n",
        "                       'relationship', 'Clothing', 'Gender', 'native-country']\r\n",
        "NUMERIC_COLUMNS = ['education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\r\n",
        "\r\n",
        "def one_hot_cat_column(feature_name, vocab):\r\n",
        "  return tf.feature_column.indicator_column(\r\n",
        "      tf.feature_column.categorical_column_with_vocabulary_list(feature_name,\r\n",
        "                                                 vocab))\r\n",
        "feature_columns = []\r\n",
        "for feature_name in CATEGORICAL_COLUMNS:\r\n",
        "  # Need to one-hot encode categorical features.\r\n",
        "  vocabulary = dftrain[feature_name].unique()\r\n",
        "  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\r\n",
        "\r\n",
        "for feature_name in NUMERIC_COLUMNS:\r\n",
        "  feature_columns.append(tf.feature_column.numeric_column(feature_name,\r\n",
        "                                           dtype=tf.float32))\r\n",
        "\t\t\t\t\t\t\t\t\t\t   \r\n",
        "\t\t\t\t\t\t\t\t\t\t   \r\n",
        "example = dict(dftrain.head(1))\r\n",
        "Gender_fc = tf.feature_column.indicator_column(tf.feature_column.categorical_column_with_vocabulary_list('Gender', ('Male', 'Female')))\r\n",
        "print('Feature value: \"{}\"'.format(example['Gender'].iloc[0]))\r\n",
        "print('One-hot encoded: ', tf.keras.layers.DenseFeatures([Gender_fc])(example).numpy())\r\n",
        "\r\n",
        "\r\n",
        "tf.keras.layers.DenseFeatures(feature_columns)(example).numpy()\r\n",
        "\r\n",
        "# Using entire batch since this is such a small dataset.\r\n",
        "NUM_EXAMPLES = len(y_train)\r\n",
        "\r\n",
        "def make_input_fn(X, y, n_epochs=None, shuffle=True):\r\n",
        "  def input_fn():\r\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\r\n",
        "    if shuffle:\r\n",
        "      dataset = dataset.shuffle(NUM_EXAMPLES)\r\n",
        "    # For training, cycle thru dataset as many times as need (n_epochs=None).\r\n",
        "    dataset = dataset.repeat(n_epochs)\r\n",
        "    # In memory training doesn't use batching.\r\n",
        "    dataset = dataset.batch(NUM_EXAMPLES)\r\n",
        "    return dataset\r\n",
        "  return input_fn\r\n",
        "\r\n",
        "# Training and evaluation input functions.\r\n",
        "train_input_fn = make_input_fn(dftrain, y_train)\r\n",
        "eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)\r\n",
        "\r\n",
        "\r\n",
        "linear_est = tf.estimator.LinearClassifier(feature_columns)\r\n",
        "\r\n",
        "# Training model.\r\n",
        "linear_est.train(train_input_fn, max_steps=100)\r\n",
        "\r\n",
        "# Evaluation.\r\n",
        "result = linear_est.evaluate(eval_input_fn)\r\n",
        "clear_output()\r\n",
        "print(pd.Series(result))\r\n",
        "\r\n",
        "# Since data fits into memory, using entire dataset per layer. It's faster.\r\n",
        "# Above one batch is defined as the entire dataset.\r\n",
        "n_batches = 1\r\n",
        "est = tf.estimator.BoostedTreesClassifier(feature_columns,\r\n",
        "                                          n_batches_per_layer=n_batches)\r\n",
        "\r\n",
        "# The model will stop training once the specified number of trees is built, not\r\n",
        "# based on the number of steps.\r\n",
        "est.train(train_input_fn, max_steps=100)\r\n",
        "\r\n",
        "# Eval.\r\n",
        "result = est.evaluate(eval_input_fn)\r\n",
        "clear_output()\r\n",
        "print(pd.Series(result))\r\n",
        "\r\n",
        "pred_dicts = list(est.predict(eval_input_fn))\r\n",
        "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\r\n",
        "\r\n",
        "probs.plot(kind='hist', bins=20, title='predicted probabilities')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "from sklearn.metrics import roc_curve\r\n",
        "\r\n",
        "fpr, tpr, _ = roc_curve(y_eval, probs)\r\n",
        "plt.plot(fpr, tpr)\r\n",
        "plt.title('ROC curve')\r\n",
        "plt.xlabel('false positive rate')\r\n",
        "plt.ylabel('true positive rate')\r\n",
        "plt.xlim(0,)\r\n",
        "plt.ylim(0,)\r\n",
        "plt.show()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PU5-B_QJQIz0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2L5oQT8QI5S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnDPHSlVQtnY"
      },
      "source": [
        "Programmatic case 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53P4cF2lnI_j"
      },
      "source": [
        "######################################################################################################################\r\n",
        "######################################################################################################################\r\n",
        "##2) Prediction 2 - Advanced Program \r\n",
        "\r\n",
        "#2.1) Tensorflow GBM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVHqa96SnVXm"
      },
      "source": [
        "!pip install statsmodels\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from IPython.display import clear_output\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "\r\n",
        "train_path = '/content/census_data.csv'\r\n",
        "\r\n",
        "train = pd.read_csv(train_path)\r\n",
        "\r\n",
        "train[\"income_level_cat2\"] = train[\"income_level_cat\"].astype('category')\r\n",
        "train.dtypes\r\n",
        "\r\n",
        "train[\"income_level_cat2\"] = train[\"income_level_cat2\"].cat.codes\r\n",
        "train.head()\r\n",
        "\r\n",
        "# split into train test sets\r\n",
        "X = train\r\n",
        "X_train, X_test= train_test_split(X, train_size=0.70)\r\n",
        "print(X_train.shape, X_test.shape)\r\n",
        "\r\n",
        "# split features and dependent\r\n",
        "y_train = X_train[\"income_level_cat2\"]\r\n",
        "dftrain = X_train.drop(\"income_level_cat2\", axis=1)\r\n",
        "y_eval = X_test[\"income_level_cat2\"]\r\n",
        "dfeval = X_test.drop(\"income_level_cat2\", axis=1)\r\n",
        "\r\n",
        "import tensorflow as tf\r\n",
        "tf.random.set_seed(123)\r\n",
        "\r\n",
        "fc = tf.feature_column\r\n",
        "CATEGORICAL_COLUMNS = ['workclass', 'fnlwgt', 'education', 'marital-status', 'occupation',\r\n",
        "                       'relationship', 'Clothing', 'Gender', 'native-country']\r\n",
        "NUMERIC_COLUMNS = ['education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\r\n",
        "\r\n",
        "def one_hot_cat_column(feature_name, vocab):\r\n",
        "  return fc.indicator_column(\r\n",
        "      fc.categorical_column_with_vocabulary_list(feature_name,\r\n",
        "                                                 vocab))\r\n",
        "feature_columns = []\r\n",
        "for feature_name in CATEGORICAL_COLUMNS:\r\n",
        "  # One-hot encoding categorical features.\r\n",
        "  vocabulary = dftrain[feature_name].unique()\r\n",
        "  feature_columns.append(one_hot_cat_column(feature_name, vocabulary))\r\n",
        "\r\n",
        "for feature_name in NUMERIC_COLUMNS:\r\n",
        "  feature_columns.append(fc.numeric_column(feature_name,\r\n",
        "                                           dtype=tf.float32))\r\n",
        "\r\n",
        "# Using entire batch since this is such a small dataset.\r\n",
        "NUM_EXAMPLES = len(y_train)\r\n",
        "\r\n",
        "def make_input_fn(X, y, n_epochs=None, shuffle=True):\r\n",
        "  def input_fn():\r\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X.to_dict(orient='list'), y))\r\n",
        "    if shuffle:\r\n",
        "      dataset = dataset.shuffle(NUM_EXAMPLES)\r\n",
        "    # For training, cycle thru dataset as many times as need (n_epochs=None).\r\n",
        "    dataset = (dataset\r\n",
        "      .repeat(n_epochs)\r\n",
        "      .batch(NUM_EXAMPLES))\r\n",
        "    return dataset\r\n",
        "  return input_fn\r\n",
        "\r\n",
        "# Training and evaluation input functions.\r\n",
        "train_input_fn = make_input_fn(dftrain, y_train)\r\n",
        "eval_input_fn = make_input_fn(dfeval, y_eval, shuffle=False, n_epochs=1)\r\n",
        "\r\n",
        "\r\n",
        "params = {\r\n",
        "  'n_trees': 50,\r\n",
        "  'max_depth': 3,\r\n",
        "  'n_batches_per_layer': 1,\r\n",
        "  # Enabling center_bias = True to get DFCs. This will force the model to\r\n",
        "  # make an initial prediction before using any features (e.g. using the mean of\r\n",
        "  # the training labels for regression or log odds for classification when\r\n",
        "  # using cross entropy loss).\r\n",
        "  'center_bias': True\r\n",
        "}\r\n",
        "\r\n",
        "est = tf.estimator.BoostedTreesClassifier(feature_columns, **params)\r\n",
        "# Train model.\r\n",
        "est.train(train_input_fn, max_steps=100)\r\n",
        "\r\n",
        "# Evaluation.\r\n",
        "results = est.evaluate(eval_input_fn)\r\n",
        "clear_output()\r\n",
        "pd.Series(results).to_frame()\r\n",
        "\r\n",
        "\r\n",
        "in_memory_params = dict(params)\r\n",
        "in_memory_params['n_batches_per_layer'] = 1\r\n",
        "# In-memory input_fn does not use batching.\r\n",
        "def make_inmemory_train_input_fn(X, y):\r\n",
        "  y = np.expand_dims(y, axis=1)\r\n",
        "  def input_fn():\r\n",
        "    return dict(X), y\r\n",
        "  return input_fn\r\n",
        "train_input_fn = make_inmemory_train_input_fn(dftrain, y_train)\r\n",
        "\r\n",
        "# Training the model.\r\n",
        "est = tf.estimator.BoostedTreesClassifier(\r\n",
        "    feature_columns, \r\n",
        "    train_in_memory=True, \r\n",
        "    **in_memory_params)\r\n",
        "\r\n",
        "est.train(train_input_fn)\r\n",
        "print(est.evaluate(eval_input_fn))\r\n",
        "\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "sns_colors = sns.color_palette('colorblind')\r\n",
        "\r\n",
        "pred_dicts = list(est.experimental_predict_with_explanations(eval_input_fn))\r\n",
        "\r\n",
        "# Creating DFC Pandas dataframe.\r\n",
        "labels = y_eval.values\r\n",
        "probs = pd.Series([pred['probabilities'][1] for pred in pred_dicts])\r\n",
        "df_dfc = pd.DataFrame([pred['dfc'] for pred in pred_dicts])\r\n",
        "df_dfc.describe().T\r\n",
        "\r\n",
        "# Sum of DFCs + bias == probabality.\r\n",
        "bias = pred_dicts[0]['bias']\r\n",
        "dfc_prob = df_dfc.sum(axis=1) + bias\r\n",
        "np.testing.assert_almost_equal(dfc_prob.values,\r\n",
        "                               probs.values)\r\n",
        "\t\t\t\t\t\t\t   \r\n",
        "# Boilerplate code for plotting \r\n",
        "def _get_color(value):\r\n",
        "    \"\"\"To make positive DFCs plot green, negative DFCs plot red.\"\"\"\r\n",
        "    green, red = sns.color_palette()[2:4]\r\n",
        "    if value >= 0: return green\r\n",
        "    return red\r\n",
        "\r\n",
        "def _add_feature_values(feature_values, ax):\r\n",
        "    \"\"\"Display feature's values on left of plot.\"\"\"\r\n",
        "    x_coord = ax.get_xlim()[0]\r\n",
        "    OFFSET = 0.15\r\n",
        "    for y_coord, (feat_name, feat_val) in enumerate(feature_values.items()):\r\n",
        "        t = plt.text(x_coord, y_coord - OFFSET, '{}'.format(feat_val), size=12)\r\n",
        "        t.set_bbox(dict(facecolor='white', alpha=0.5))\r\n",
        "    from matplotlib.font_manager import FontProperties\r\n",
        "    font = FontProperties()\r\n",
        "    font.set_weight('bold')\r\n",
        "    t = plt.text(x_coord, y_coord + 1 - OFFSET, 'feature\\nvalue',\r\n",
        "    fontproperties=font, size=12)\r\n",
        "\r\n",
        "def plot_example(example):\r\n",
        "  TOP_N = 8 # View top 8 features.\r\n",
        "  sorted_ix = example.abs().sort_values()[-TOP_N:].index  # Sort by magnitude.\r\n",
        "  example = example[sorted_ix]\r\n",
        "  colors = example.map(_get_color).tolist()\r\n",
        "  ax = example.to_frame().plot(kind='barh',\r\n",
        "                          color=[colors],\r\n",
        "                          legend=None,\r\n",
        "                          alpha=0.75,\r\n",
        "                          figsize=(10,6))\r\n",
        "  ax.grid(False, axis='y')\r\n",
        "  ax.set_yticklabels(ax.get_yticklabels(), size=14)\r\n",
        "\r\n",
        "  # Adding feature values.\r\n",
        "  _add_feature_values(dfeval.iloc[ID][sorted_ix], ax)\r\n",
        "  return ax\r\n",
        "  \r\n",
        "  \r\n",
        "# Plotting results.\r\n",
        "ID = 182\r\n",
        "example = df_dfc.iloc[ID]  # Choose ith example from evaluation set.\r\n",
        "TOP_N = 8  # View top 8 features.\r\n",
        "sorted_ix = example.abs().sort_values()[-TOP_N:].index\r\n",
        "ax = plot_example(example)\r\n",
        "ax.set_title('Feature contributions for example {}\\n pred: {:1.2f}; label: {}'.format(ID, probs[ID], labels[ID]))\r\n",
        "ax.set_xlabel('Contribution to predicted probability', size=14)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "# Boilerplate plotting code.\r\n",
        "def dist_violin_plot(df_dfc, ID):\r\n",
        "  # Initializing plot.\r\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(10, 6))\r\n",
        "\r\n",
        "  # Creating example dataframe.\r\n",
        "  TOP_N = 8  # View top 8 features.\r\n",
        "  example = df_dfc.iloc[ID]\r\n",
        "  ix = example.abs().sort_values()[-TOP_N:].index\r\n",
        "  example = example[ix]\r\n",
        "  example_df = example.to_frame(name='dfc')\r\n",
        "\r\n",
        "  # Adding contributions of entire distribution.\r\n",
        "  parts=ax.violinplot([df_dfc[w] for w in ix],\r\n",
        "                 vert=False,\r\n",
        "                 showextrema=False,\r\n",
        "                 widths=0.7,\r\n",
        "                 positions=np.arange(len(ix)))\r\n",
        "  face_color = sns_colors[0]\r\n",
        "  alpha = 0.15\r\n",
        "  for pc in parts['bodies']:\r\n",
        "      pc.set_facecolor(face_color)\r\n",
        "      pc.set_alpha(alpha)\r\n",
        "\r\n",
        "  # Adding feature values.\r\n",
        "  _add_feature_values(dfeval.iloc[ID][sorted_ix], ax)\r\n",
        "\r\n",
        "  # Adding local contributions.\r\n",
        "  ax.scatter(example,\r\n",
        "              np.arange(example.shape[0]),\r\n",
        "              color=sns.color_palette()[2],\r\n",
        "              s=100,\r\n",
        "              marker=\"s\",\r\n",
        "              label='contributions for example')\r\n",
        "\r\n",
        "\r\n",
        "  # Proxy plot, to show violinplot dist on legend.\r\n",
        "  ax.plot([0,0], [1,1], label='eval set contributions\\ndistributions',\r\n",
        "          color=face_color, alpha=alpha, linewidth=10)\r\n",
        "  legend = ax.legend(loc='lower right', shadow=True, fontsize='x-large',\r\n",
        "                     frameon=True)\r\n",
        "  legend.get_frame().set_facecolor('white')\r\n",
        "\r\n",
        "  # Formatting plot.\r\n",
        "  ax.set_yticks(np.arange(example.shape[0]))\r\n",
        "  ax.set_yticklabels(example.index)\r\n",
        "  ax.grid(False, axis='y')\r\n",
        "  ax.set_xlabel('Contribution to predicted probability', size=14)\r\n",
        "    \r\n",
        "dist_violin_plot(df_dfc, ID)\r\n",
        "plt.title('Feature contributions for example {}\\n pred: {:1.2f}; label: {}'.format(ID, probs[ID], labels[ID]))\r\n",
        "plt.show() \r\n",
        "\r\n",
        "importances = est.experimental_feature_importances(normalize=True)\r\n",
        "df_imp = pd.Series(importances)\r\n",
        "\r\n",
        "# Visualizing importances.\r\n",
        "N = 8\r\n",
        "ax = (df_imp.iloc[0:N][::-1]\r\n",
        "    .plot(kind='barh',\r\n",
        "          color=sns_colors[0],\r\n",
        "          title='Gain feature importances',\r\n",
        "          figsize=(10, 6)))\r\n",
        "ax.grid(False, axis='y')\r\n",
        "\r\n",
        "# Plot\r\n",
        "dfc_mean = df_dfc.abs().mean()\r\n",
        "N = 8\r\n",
        "sorted_ix = dfc_mean.abs().sort_values()[-N:].index  # Average and sort by absolute.\r\n",
        "ax = dfc_mean[sorted_ix].plot(kind='barh',\r\n",
        "                       color=sns_colors[1],\r\n",
        "                       title='Mean |directional feature contributions|',\r\n",
        "                       figsize=(10, 6))\r\n",
        "ax.grid(False, axis='y')\r\n",
        "\r\n",
        "\r\n",
        "FEATURE = 'fare'\r\n",
        "feature = pd.Series(df_dfc[FEATURE].values, index=dfeval[FEATURE].values).sort_index()\r\n",
        "ax = sns.regplot(feature.index.values, feature.values, lowess=True)\r\n",
        "ax.set_ylabel('contribution')\r\n",
        "ax.set_xlabel(FEATURE)\r\n",
        "ax.set_xlim(0, 100)\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "def permutation_importances(est, X_eval, y_eval, metric, features):\r\n",
        "    \"\"\"Column by column, shuffle values and observe effect on eval set.\r\n",
        "\r\n",
        "    source: http://explained.ai/rf-importance/index.html\r\n",
        "    A similar approach can be done during training. See \"Drop-column importance\"\r\n",
        "    in the above article.\"\"\"\r\n",
        "    baseline = metric(est, X_eval, y_eval)\r\n",
        "    imp = []\r\n",
        "    for col in features:\r\n",
        "        save = X_eval[col].copy()\r\n",
        "        X_eval[col] = np.random.permutation(X_eval[col])\r\n",
        "        m = metric(est, X_eval, y_eval)\r\n",
        "        X_eval[col] = save\r\n",
        "        imp.append(baseline - m)\r\n",
        "    return np.array(imp)\r\n",
        "\r\n",
        "def accuracy_metric(est, X, y):\r\n",
        "    \"\"\"TensorFlow estimator accuracy.\"\"\"\r\n",
        "    eval_input_fn = make_input_fn(X,\r\n",
        "                                  y=y,\r\n",
        "                                  shuffle=False,\r\n",
        "                                  n_epochs=1)\r\n",
        "    return est.evaluate(input_fn=eval_input_fn)['accuracy']\r\n",
        "features = CATEGORICAL_COLUMNS + NUMERIC_COLUMNS\r\n",
        "importances = permutation_importances(est, dfeval, y_eval, accuracy_metric,\r\n",
        "                                      features)\r\n",
        "df_imp = pd.Series(importances, index=features)\r\n",
        "\r\n",
        "sorted_ix = df_imp.abs().sort_values().index\r\n",
        "ax = df_imp[sorted_ix][-5:].plot(kind='barh', color=sns_colors[2], figsize=(10, 6))\r\n",
        "ax.grid(False, axis='y')\r\n",
        "ax.set_title('Permutation feature importance')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "from numpy.random import uniform, seed\r\n",
        "from scipy.interpolate import griddata\r\n",
        "\r\n",
        "# Creating sample data\r\n",
        "seed(0)\r\n",
        "npts = 5000\r\n",
        "x = uniform(-2, 2, npts)\r\n",
        "y = uniform(-2, 2, npts)\r\n",
        "z = x*np.exp(-x**2 - y**2)\r\n",
        "xy = np.zeros((2,np.size(x)))\r\n",
        "xy[0] = x\r\n",
        "xy[1] = y\r\n",
        "xy = xy.T\r\n",
        "\r\n",
        "# Prepping data for training.\r\n",
        "df = pd.DataFrame({'x': x, 'y': y, 'z': z})\r\n",
        "\r\n",
        "xi = np.linspace(-2.0, 2.0, 200),\r\n",
        "yi = np.linspace(-2.1, 2.1, 210),\r\n",
        "xi,yi = np.meshgrid(xi, yi)\r\n",
        "\r\n",
        "df_predict = pd.DataFrame({\r\n",
        "    'x' : xi.flatten(),\r\n",
        "    'y' : yi.flatten(),\r\n",
        "})\r\n",
        "predict_shape = xi.shape\r\n",
        "\r\n",
        "\r\n",
        "def plot_contour(x, y, z, **kwargs):\r\n",
        "  # Gridding the data.\r\n",
        "  plt.figure(figsize=(10, 8))\r\n",
        "  # Contouring the gridded data, plotting dots at the nonuniform data points.\r\n",
        "  CS = plt.contour(x, y, z, 15, linewidths=0.5, colors='k')\r\n",
        "  CS = plt.contourf(x, y, z, 15,\r\n",
        "                    vmax=abs(zi).max(), vmin=-abs(zi).max(), cmap='RdBu_r')\r\n",
        "  plt.colorbar()  # Draw colorbar.\r\n",
        "  # Plotting data points.\r\n",
        "  plt.xlim(-2, 2)\r\n",
        "  plt.ylim(-2, 2)\r\n",
        "  \r\n",
        "  \r\n",
        "zi = griddata(xy, z, (xi, yi), method='linear', fill_value='0')\r\n",
        "plot_contour(xi, yi, zi)\r\n",
        "plt.scatter(df.x, df.y, marker='.')\r\n",
        "plt.title('Contour on training data')\r\n",
        "plt.show()\r\n",
        "\r\n",
        "\r\n",
        "fc = [tf.feature_column.numeric_column('x'),\r\n",
        "      tf.feature_column.numeric_column('y')]\r\n",
        "\r\n",
        "\r\n",
        "def predict(est):\r\n",
        "  \"\"\"Predictions from a given estimator.\"\"\"\r\n",
        "  predict_input_fn = lambda: tf.data.Dataset.from_tensors(dict(df_predict))\r\n",
        "  preds = np.array([p['predictions'][0] for p in est.predict(predict_input_fn)])\r\n",
        "  return preds.reshape(predict_shape)\r\n",
        "\r\n",
        "\r\n",
        "train_input_fn = make_input_fn(df, df.z)\r\n",
        "est = tf.estimator.LinearRegressor(fc)\r\n",
        "est.train(train_input_fn, max_steps=500);\r\n",
        "\r\n",
        "plot_contour(xi, yi, predict(est))\r\n",
        "\t\t\t\t\t\t\t\t\t   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoSPu4Adnh_X"
      },
      "source": [
        "n_trees = 37 #@param {type: \"slider\", min: 1, max: 80, step: 1}\r\n",
        "\r\n",
        "est = tf.estimator.BoostedTreesRegressor(fc, n_batches_per_layer=1, n_trees=n_trees)\r\n",
        "est.train(train_input_fn, max_steps=500)\r\n",
        "clear_output()\r\n",
        "plot_contour(xi, yi, predict(est))\r\n",
        "plt.text(-1.8, 2.1, '# trees: {}'.format(n_trees), color='w', backgroundcolor='black', size=20)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}