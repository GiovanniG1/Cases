{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced_programming_8_Case_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GYDC0IX8-mf"
      },
      "source": [
        "## Confidentiality\r\n",
        "\r\n",
        "The programmatic cases in this notebook are utilized from different internet resources (in this notebook especially from kaggle.com) and are for demonstrational purposes only.\r\n",
        "\r\n",
        "Please do not copy or distribute this notebook.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1VoV6ir9EEY"
      },
      "source": [
        "## Table of content\r\n",
        "\r\n",
        "Porto Seguro Safe Driver\r\n",
        "\r\n",
        "1. Programmatic case 1 \r\n",
        "2. Programmatic case 2\r\n",
        "3. Programmatic case 3\r\n",
        "4. Programmatic case 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iptd_m3GLYDT"
      },
      "source": [
        "## Introduction\r\n",
        "\r\n",
        "The programmatic cases in this notebook are very advanced. The goal of this notebook should be to keep increasing a detailed understanding by repetitive analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_SB6PLi9HSj"
      },
      "source": [
        "## Previous knowledge\r\n",
        "\r\n",
        "For a good understanding of this notebook you should have a few years of data-science and programming experience and have studied the advanced programming notebooks.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66-RuzWk37cq"
      },
      "source": [
        "#### Programmatic case 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzu9AZlaUVhq"
      },
      "source": [
        "from google.colab import files\r\n",
        "import io\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "\r\n",
        "from xgboost import XGBClassifier\r\n",
        "from lightgbm import LGBMClassifier\r\n",
        "\r\n",
        "!pip install catboost\r\n",
        "!pip install ipywidgets\r\n",
        "!jupyter nbextension enable --py widgetsnbextension\r\n",
        "from catboost import CatBoostClassifier\r\n",
        "\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\r\n",
        "\r\n",
        "# Regularized Greedy Forest\r\n",
        "!pip install rgf_python -v\r\n",
        "from rgf.sklearn import RGFClassifier     # https://github.com/fukatani/rgf_python\r\n",
        "\r\n",
        "\r\n",
        "uploaded = files.upload()\r\n",
        "\r\n",
        "\r\n",
        "train = pd.read_csv(io.BytesIO(uploaded['train_short_version.csv']))\r\n",
        "test = pd.read_csv(io.BytesIO(uploaded['test_short_version.csv']))\r\n",
        "\r\n",
        "\r\n",
        "# Preprocessing \r\n",
        "id_test = test['id'].values\r\n",
        "target_train = train['target'].values\r\n",
        "\r\n",
        "train = train.drop(['target','id'], axis = 1)\r\n",
        "test = test.drop(['id'], axis = 1)\r\n",
        "\r\n",
        "\r\n",
        "col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\r\n",
        "train = train.drop(col_to_drop, axis=1)  \r\n",
        "test = test.drop(col_to_drop, axis=1)  \r\n",
        "\r\n",
        "\r\n",
        "train = train.replace(-1, np.nan)\r\n",
        "test = test.replace(-1, np.nan)\r\n",
        "\r\n",
        "\r\n",
        "cat_features = [a for a in train.columns if a.endswith('cat')]\r\n",
        "\r\n",
        "for column in cat_features:\r\n",
        "\ttemp = pd.get_dummies(pd.Series(train[column]))\r\n",
        "\ttrain = pd.concat([train,temp],axis=1)\r\n",
        "\ttrain = train.drop([column],axis=1)\r\n",
        "    \r\n",
        "for column in cat_features:\r\n",
        "\ttemp = pd.get_dummies(pd.Series(test[column]))\r\n",
        "\ttest = pd.concat([test,temp],axis=1)\r\n",
        "\ttest = test.drop([column],axis=1)\r\n",
        "\r\n",
        "\r\n",
        "print(train.values.shape, test.values.shape)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class Ensemble(object):\r\n",
        "    def __init__(self, n_splits, stacker, base_models):\r\n",
        "        self.n_splits = n_splits\r\n",
        "        self.stacker = stacker\r\n",
        "        self.base_models = base_models\r\n",
        "\r\n",
        "    def fit_predict(self, X, y, T):\r\n",
        "        X = np.array(X)\r\n",
        "        y = np.array(y)\r\n",
        "        T = np.array(T)\r\n",
        "\r\n",
        "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=2016).split(X, y))\r\n",
        "\r\n",
        "        S_train = np.zeros((X.shape[0], len(self.base_models)))\r\n",
        "        S_test = np.zeros((T.shape[0], len(self.base_models)))\r\n",
        "        for i, clf in enumerate(self.base_models):\r\n",
        "\r\n",
        "            S_test_i = np.zeros((T.shape[0], self.n_splits))\r\n",
        "\r\n",
        "            for j, (train_idx, test_idx) in enumerate(folds):\r\n",
        "                X_train = X[train_idx]\r\n",
        "                y_train = y[train_idx]\r\n",
        "                X_holdout = X[test_idx]\r\n",
        "#                y_holdout = y[test_idx]\r\n",
        "\r\n",
        "                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\r\n",
        "                clf.fit(X_train, y_train)\r\n",
        "#                cross_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc')\r\n",
        "#                print(\"    cross_score: %.5f\" % (cross_score.mean()))\r\n",
        "                y_pred = clf.predict_proba(X_holdout)[:,1]                \r\n",
        "\r\n",
        "                S_train[test_idx, i] = y_pred\r\n",
        "                S_test_i[:, j] = clf.predict_proba(T)[:,1]\r\n",
        "            S_test[:, i] = S_test_i.mean(axis=1)\r\n",
        "\r\n",
        "        results = cross_val_score(self.stacker, S_train, y, cv=3, scoring='roc_auc')\r\n",
        "        print(\"Stacker score: %.5f\" % (results.mean()))\r\n",
        "\r\n",
        "        self.stacker.fit(S_train, y)\r\n",
        "        res = self.stacker.predict_proba(S_test)[:,1]\r\n",
        "        return res\r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "# LightGBM params\r\n",
        "lgb_params = {}\r\n",
        "lgb_params['learning_rate'] = 0.02\r\n",
        "lgb_params['n_estimators'] = 650\r\n",
        "lgb_params['max_bin'] = 10\r\n",
        "lgb_params['subsample'] = 0.8\r\n",
        "lgb_params['subsample_freq'] = 10\r\n",
        "lgb_params['colsample_bytree'] = 0.8   \r\n",
        "lgb_params['min_child_samples'] = 500\r\n",
        "lgb_params['seed'] = 99\r\n",
        "\r\n",
        "\r\n",
        "lgb_params2 = {}\r\n",
        "lgb_params2['n_estimators'] = 1090\r\n",
        "lgb_params2['learning_rate'] = 0.02\r\n",
        "lgb_params2['colsample_bytree'] = 0.3   \r\n",
        "lgb_params2['subsample'] = 0.7\r\n",
        "lgb_params2['subsample_freq'] = 2\r\n",
        "lgb_params2['num_leaves'] = 16\r\n",
        "lgb_params2['seed'] = 99\r\n",
        "\r\n",
        "\r\n",
        "lgb_params3 = {}\r\n",
        "lgb_params3['n_estimators'] = 1100\r\n",
        "lgb_params3['max_depth'] = 4\r\n",
        "lgb_params3['learning_rate'] = 0.02\r\n",
        "lgb_params3['seed'] = 99\r\n",
        "\r\n",
        "\r\n",
        "lgb_model = LGBMClassifier(**lgb_params)\r\n",
        "\r\n",
        "lgb_model2 = LGBMClassifier(**lgb_params2)\r\n",
        "\r\n",
        "lgb_model3 = LGBMClassifier(**lgb_params3)\r\n",
        "\r\n",
        "\r\n",
        "log_model = LogisticRegression()\r\n",
        "\r\n",
        "        \r\n",
        "stack = Ensemble(n_splits=3,\r\n",
        "        stacker = log_model,\r\n",
        "        base_models = (lgb_model, lgb_model2, lgb_model3))        \r\n",
        "        \r\n",
        "y_pred = stack.fit_predict(train, target_train, test)        \r\n",
        "\r\n",
        "\r\n",
        "sub = pd.DataFrame()\r\n",
        "sub['id'] = id_test\r\n",
        "sub['target'] = y_pred\r\n",
        "sub.to_csv('stacked_1.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9p8lx8SUWUF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXudDdLQdC3V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3tIAjLB1uG0"
      },
      "source": [
        "#### Programmatic case 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IGJ98KA1mKS"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "\r\n",
        "from lightgbm import LGBMClassifier\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.ensemble import ExtraTreesClassifier\r\n",
        "from sklearn.neural_network import MLPClassifier\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "\r\n",
        "import io\r\n",
        "import pandas as pd\r\n",
        "train = pd.read_csv(io.BytesIO(uploaded['train_short_version.csv']))\r\n",
        "test = pd.read_csv(io.BytesIO(uploaded['test_short_version.csv']))\r\n",
        "\r\n",
        "\r\n",
        "# Preprocessing (remove ps_calc cols, replace -1 with NaN, OHE categorical features)\r\n",
        "id_test = test['id'].values\r\n",
        "target_train = train['target'].values\r\n",
        "\r\n",
        "train = train.drop(['target','id'], axis = 1)\r\n",
        "test = test.drop(['id'], axis = 1)\r\n",
        "\r\n",
        "col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\r\n",
        "train = train.drop(col_to_drop, axis=1)  \r\n",
        "test = test.drop(col_to_drop, axis=1)  \r\n",
        "\r\n",
        "train = train.replace(-1, np.nan)\r\n",
        "test = test.replace(-1, np.nan)\r\n",
        "\r\n",
        "cat_features = [a for a in train.columns if a.endswith('cat')]\r\n",
        "\r\n",
        "for column in cat_features:\r\n",
        "    temp = pd.get_dummies(pd.Series(train[column]))\r\n",
        "    train = pd.concat([train,temp],axis=1)\r\n",
        "    train = train.drop([column],axis=1)\r\n",
        "    \r\n",
        "for column in cat_features:\r\n",
        "    temp = pd.get_dummies(pd.Series(test[column]))\r\n",
        "    test = pd.concat([test,temp],axis=1)\r\n",
        "    test = test.drop([column],axis=1)\r\n",
        "\r\n",
        "print(train.values.shape, test.values.shape)\r\n",
        "\r\n",
        "\r\n",
        "class Ensemble(object):    \r\n",
        "    def __init__(self, mode, n_splits, stacker_2, stacker_1, base_models):\r\n",
        "        self.mode = mode\r\n",
        "        self.n_splits = n_splits\r\n",
        "        self.stacker_2 = stacker_2\r\n",
        "        self.stacker_1 = stacker_1\r\n",
        "        self.base_models = base_models\r\n",
        "\r\n",
        "    def fit_predict(self, X, y, T):\r\n",
        "        X = np.array(X)\r\n",
        "        y = np.array(y)\r\n",
        "        T = np.array(T)\r\n",
        "\r\n",
        "\r\n",
        "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, \r\n",
        "                                                             random_state=2016).split(X, y))\r\n",
        "        \r\n",
        "        OOF_columns = []\r\n",
        "\r\n",
        "        S_train = np.zeros((X.shape[0], len(self.base_models)))\r\n",
        "        S_test = np.zeros((T.shape[0], len(self.base_models)))\r\n",
        "        \r\n",
        "        for i, clf in enumerate(self.base_models):\r\n",
        "\r\n",
        "            S_test_i = np.zeros((T.shape[0], self.n_splits))\r\n",
        "\r\n",
        "            for j, (train_idx, test_idx) in enumerate(folds):                \r\n",
        "                X_train = X[train_idx]\r\n",
        "                y_train = y[train_idx]\r\n",
        "                X_holdout = X[test_idx]\r\n",
        "\r\n",
        "                print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\r\n",
        "                clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "                S_train[test_idx, i] = clf.predict_proba(X_holdout)[:,1]  \r\n",
        "                S_test_i[:, j] = clf.predict_proba(T)[:,1]                \r\n",
        "            S_test[:, i] = S_test_i.mean(axis=1)\r\n",
        "            \r\n",
        "            print(\"  Base model_%d score: %.5f\\n\" % (i+1, roc_auc_score(y, S_train[:,i])))\r\n",
        "        \r\n",
        "            OOF_columns.append('Base model_'+str(i+1))\r\n",
        "        OOF_S_train = pd.DataFrame(S_train, columns = OOF_columns)\r\n",
        "        print('\\n')\r\n",
        "        print('Correlation between out-of-fold predictions from Base models:')\r\n",
        "        print('\\n')\r\n",
        "        print(OOF_S_train.corr())\r\n",
        "        print('\\n')\r\n",
        "            \r\n",
        "        \r\n",
        "        if self.mode==1:\r\n",
        "            \r\n",
        "            folds_2 = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True,\r\n",
        "                                                                   random_state=2016).split(S_train, y))\r\n",
        "            \r\n",
        "            OOF_columns = []\r\n",
        "\r\n",
        "            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\r\n",
        "            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\r\n",
        "            \r\n",
        "            for i, clf in enumerate(self.stacker_1):\r\n",
        "            \r\n",
        "                S_test_i_2 = np.zeros((S_test.shape[0], self.n_splits))\r\n",
        "\r\n",
        "                for j, (train_idx, test_idx) in enumerate(folds_2):\r\n",
        "                    X_train_2 = S_train[train_idx]\r\n",
        "                    y_train_2 = y[train_idx]\r\n",
        "                    X_holdout_2 = S_train[test_idx]\r\n",
        "\r\n",
        "                    print (\"Fit %s_%d fold %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\r\n",
        "                    clf.fit(X_train_2, y_train_2)\r\n",
        "                                 \r\n",
        "                    S_train_2[test_idx, i] = clf.predict_proba(X_holdout_2)[:,1] \r\n",
        "                    S_test_i_2[:, j] = clf.predict_proba(S_test)[:,1]\r\n",
        "                S_test_2[:, i] = S_test_i_2.mean(axis=1)\r\n",
        "                \r\n",
        "                print(\"  1st level model_%d score: %.5f\\n\"%(i+1,\r\n",
        "                                                            roc_auc_score(y, S_train_2.mean(axis=1))))\r\n",
        "                \r\n",
        "                OOF_columns.append('1st level model_'+str(i+1))\r\n",
        "            OOF_S_train = pd.DataFrame(S_train_2, columns = OOF_columns)\r\n",
        "            print('\\n')\r\n",
        "            print('Correlation between out-of-fold predictions from 1st level models:')\r\n",
        "            print('\\n')\r\n",
        "            print(OOF_S_train.corr())\r\n",
        "            print('\\n')\r\n",
        "\r\n",
        "\r\n",
        "        if self.mode==2:\r\n",
        "            \r\n",
        "            WOC_columns = []\r\n",
        "        \r\n",
        "            S_train_2 = np.zeros((S_train.shape[0], len(self.stacker_1)))\r\n",
        "            S_test_2 = np.zeros((S_test.shape[0], len(self.stacker_1)))\r\n",
        "               \r\n",
        "            for i, clf in enumerate(self.stacker_1):\r\n",
        "            \r\n",
        "                S_train_i_2= np.zeros((S_train.shape[0], S_train.shape[1]))\r\n",
        "                S_test_i_2 = np.zeros((S_test.shape[0], S_train.shape[1]))\r\n",
        "                                       \r\n",
        "                for j in range(S_train.shape[1]):\r\n",
        "                                \r\n",
        "                    S_tr = S_train[:,np.arange(S_train.shape[1])!=j]\r\n",
        "                    S_te = S_test[:,np.arange(S_test.shape[1])!=j]\r\n",
        "                                               \r\n",
        "                    print (\"Fit %s_%d subset %d\" % (str(clf).split(\"(\")[0], i+1, j+1))\r\n",
        "                    clf.fit(S_tr, y)\r\n",
        "\r\n",
        "                    S_train_i_2[:, j] = clf.predict_proba(S_tr)[:,1]                \r\n",
        "                    S_test_i_2[:, j] = clf.predict_proba(S_te)[:,1]\r\n",
        "                S_train_2[:, i] = S_train_i_2.mean(axis=1)    \r\n",
        "                S_test_2[:, i] = S_test_i_2.mean(axis=1)\r\n",
        "            \r\n",
        "                print(\"  1st level model_%d score: %.5f\\n\"%(i+1,\r\n",
        "                                                            roc_auc_score(y, S_train_2.mean(axis=1))))\r\n",
        "                \r\n",
        "                WOC_columns.append('1st level model_'+str(i+1))\r\n",
        "            WOC_S_train = pd.DataFrame(S_train_2, columns = WOC_columns)\r\n",
        "            print('\\n')\r\n",
        "            print('Correlation between without-one-column predictions from 1st level models:')\r\n",
        "            print('\\n')\r\n",
        "            print(WOC_S_train.corr())\r\n",
        "            print('\\n')\r\n",
        "            \r\n",
        "            \r\n",
        "        try:\r\n",
        "            num_models = len(self.stacker_2)\r\n",
        "            if self.stacker_2==(et_model):\r\n",
        "                num_models=1\r\n",
        "        except TypeError:\r\n",
        "            num_models = len([self.stacker_2])\r\n",
        "            \r\n",
        "        if num_models==1:\r\n",
        "                \r\n",
        "            print (\"Fit %s for final\\n\" % (str(self.stacker_2).split(\"(\")[0]))\r\n",
        "            self.stacker_2.fit(S_train_2, y)\r\n",
        "            \r\n",
        "            stack_res = self.stacker_2.predict_proba(S_test_2)[:,1]\r\n",
        "        \r\n",
        "            stack_score = self.stacker_2.predict_proba(S_train_2)[:,1]\r\n",
        "            print(\"2nd level model final score: %.5f\" % (roc_auc_score(y, stack_score)))\r\n",
        "                \r\n",
        "        else:\r\n",
        "            \r\n",
        "            F_columns = []\r\n",
        "            \r\n",
        "            stack_score = np.zeros((S_train_2.shape[0], len(self.stacker_2)))\r\n",
        "            res = np.zeros((S_test_2.shape[0], len(self.stacker_2)))\r\n",
        "            \r\n",
        "            for i, clf in enumerate(self.stacker_2):\r\n",
        "                \r\n",
        "                print (\"Fit %s_%d\" % (str(clf).split(\"(\")[0], i+1))\r\n",
        "                clf.fit(S_train_2, y)\r\n",
        "                \r\n",
        "                stack_score[:, i] = clf.predict_proba(S_train_2)[:,1]\r\n",
        "                print(\"  2nd level model_%d score: %.5f\\n\"%(i+1,roc_auc_score(y, stack_score[:, i])))\r\n",
        "                \r\n",
        "                res[:, i] = clf.predict_proba(S_test_2)[:,1]\r\n",
        "                \r\n",
        "                F_columns.append('2nd level model_'+str(i+1))\r\n",
        "            F_S_train = pd.DataFrame(stack_score, columns = F_columns)\r\n",
        "            print('\\n')\r\n",
        "            print('Correlation between final predictions from 2nd level models:')\r\n",
        "            print('\\n')\r\n",
        "            print(F_S_train.corr())\r\n",
        "            print('\\n')\r\n",
        "        \r\n",
        "            stack_res = res.mean(axis=1)            \r\n",
        "            print(\"2nd level models final score: %.5f\" % (roc_auc_score(y, stack_score.mean(axis=1))))\r\n",
        "\r\n",
        "            \r\n",
        "        return stack_res\r\n",
        "\r\n",
        "\r\n",
        "# LightGBM params\r\n",
        "lgb_params_1 = {\r\n",
        "    'learning_rate': 0.02,\r\n",
        "    'n_estimators': 750,\r\n",
        "    'subsample': 0.8,\r\n",
        "    'subsample_freq': 10,\r\n",
        "    'colsample_bytree': 0.8,\r\n",
        "    'max_bin': 10,\r\n",
        "    'min_child_samples': 500,\r\n",
        "    'seed': 99\r\n",
        "}\r\n",
        "\r\n",
        "lgb_params_2 = {\r\n",
        "    'learning_rate': 0.02,\r\n",
        "    'n_estimators': 1200,\r\n",
        "    'subsample': 0.7,\r\n",
        "    'subsample_freq': 2,\r\n",
        "    'colsample_bytree': 0.3,  \r\n",
        "    'num_leaves': 16,\r\n",
        "    'seed': 99\r\n",
        "}\r\n",
        "\r\n",
        "lgb_params_3 = {\r\n",
        "    'learning_rate': 0.02,\r\n",
        "    'n_estimators': 475,\r\n",
        "    'subsample': 0.4,\r\n",
        "    'subsample_freq': 1,\r\n",
        "    'colsample_bytree': 0.9,  \r\n",
        "    'num_leaves': 28,\r\n",
        "    'max_bin': 10,\r\n",
        "    'min_child_samples': 700,\r\n",
        "    'seed': 99\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "# Base models\r\n",
        "lgb_model_1 = LGBMClassifier(**lgb_params_1)\r\n",
        "\r\n",
        "lgb_model_2 = LGBMClassifier(**lgb_params_2)\r\n",
        "\r\n",
        "lgb_model_3 = LGBMClassifier(**lgb_params_3)\r\n",
        "\r\n",
        "\r\n",
        "# Stacker models\r\n",
        "log_model = LogisticRegression()\r\n",
        "\r\n",
        "et_model = ExtraTreesClassifier(n_estimators=100, max_depth=6, min_samples_split=10, random_state=10)\r\n",
        "\r\n",
        "mlp_model = MLPClassifier(max_iter=7, random_state=42)\r\n",
        "\r\n",
        "\r\n",
        "# Mode 2 run\r\n",
        "stack = Ensemble(mode=2,\r\n",
        "        n_splits=3,\r\n",
        "        stacker_2 = (log_model, et_model),         \r\n",
        "        stacker_1 = (log_model, et_model, mlp_model),\r\n",
        "        base_models = (lgb_model_1, lgb_model_2, lgb_model_3))       \r\n",
        "        \r\n",
        "y_pred = stack.fit_predict(train, target_train, test) \r\n",
        "\r\n",
        "\r\n",
        "# Submission from mode 2\r\n",
        "sub = pd.DataFrame()\r\n",
        "sub['id'] = id_test\r\n",
        "sub['target'] = y_pred\r\n",
        "sub.to_csv('2-level_stacked_1.csv', index=False)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_daK5lsM1mNN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We0vOXRaUyJy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZwu04eODeOt"
      },
      "source": [
        "#### Programmatic case 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egV9o_DJDTtg"
      },
      "source": [
        "import numpy as np # linear algebra\r\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "\r\n",
        "# Input data files are available in the \"../content/\" directory.\r\n",
        "\r\n",
        "from subprocess import check_output\r\n",
        "print(check_output([\"ls\", \"../content\"]).decode(\"utf8\"))\r\n",
        "\r\n",
        "# Any results you write to the current directory are saved as output.\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "import json, math, os, sys\r\n",
        "import xgboost as xgb\r\n",
        "import gc\r\n",
        "\r\n",
        "def read_data():\r\n",
        "    root_dir = '../content/'\r\n",
        "    df_train = pd.read_csv(os.path.join(root_dir, 'train.csv'), na_values=-1)\r\n",
        "    df_train.drop([149161], axis=0, inplace=True)\r\n",
        "    df_test = pd.read_csv(os.path.join(root_dir, 'test.csv'), na_values=-1)\r\n",
        "    df_y = df_train['target']\r\n",
        "    train_id = df_train['id']\r\n",
        "    df_train.drop(['id', 'target'], axis=1, inplace=True)\r\n",
        "    df_sub = df_test['id'].to_frame()\r\n",
        "    df_sub['target'] = 0.0\r\n",
        "    df_test.drop(['id'], axis=1, inplace=True)\r\n",
        "    return df_train, df_y, df_test, df_sub, train_id\r\n",
        "\r\n",
        "def write_data(df_sub, train_id, stacker_train, sub_filename, train_filename):\r\n",
        "    df_sub.to_csv(sub_filename, index=False)\r\n",
        "    s_train = pd.DataFrame()\r\n",
        "    s_train['id'] = train_id\r\n",
        "    s_train['prob'] = stacker_train\r\n",
        "    s_train.to_csv(train_filename, index=False)\r\n",
        "\r\n",
        "class SingleXGB(object):\r\n",
        "    def __init__(self, X, y, test, skf, N):\r\n",
        "        self.X = X\r\n",
        "        self.y = y\r\n",
        "        self.test = test\r\n",
        "        self.skf = skf\r\n",
        "        self.N = N\r\n",
        "\r\n",
        "    def oof(self, params, best_rounds, sub, do_logit=True):\r\n",
        "        stacker_train = np.zeros((self.X.shape[0], 1))\r\n",
        "        dtest = xgb.DMatrix(data=self.test.values)\r\n",
        "        for index, (trn_idx, val_idx) in enumerate(self.skf.split(self.X, self.y)):\r\n",
        "            trn_x, val_x = self.X[trn_idx], self.X[val_idx]\r\n",
        "            trn_y, val_y = self.y[trn_idx], self.y[val_idx]\r\n",
        "            dtrn = xgb.DMatrix(data=trn_x, label=trn_y)\r\n",
        "            dval = xgb.DMatrix(data=val_x, label=val_y)\r\n",
        "            print('Train model in fold {0}'.format(index))\r\n",
        "            cv_model = xgb.train(\r\n",
        "                params=params,\r\n",
        "                dtrain=dtrn,\r\n",
        "                num_boost_round=best_rounds,\r\n",
        "                verbose_eval=10,\r\n",
        "            )\r\n",
        "            print('Predict in fold {0}'.format(index))\r\n",
        "            prob = cv_model.predict(dtest, ntree_limit=best_rounds)\r\n",
        "            stacker_train[val_idx,0] = cv_model.predict(dval, ntree_limit=best_rounds)\r\n",
        "            sub['target'] += prob / self.N\r\n",
        "        if do_logit:\r\n",
        "            sub['target'] = 1 / (1 + np.exp(-sub['target']))\r\n",
        "            stacker_train = 1 / (1 + np.exp(-stacker_train))\r\n",
        "        print('{0} of folds'.format(self.N))\r\n",
        "        print('Oof by single xgboost model Done')\r\n",
        "        return sub, stacker_train\r\n",
        "\r\n",
        "class Compose(object):\r\n",
        "    def __init__(self, transforms_params):\r\n",
        "        self.transforms_params = transforms_params\r\n",
        "    def __call__(self, df):\r\n",
        "        for transform_param in self.transforms_params:\r\n",
        "            transform, param = transform_param[0], transform_param[1]\r\n",
        "            df = transform(df, **param)\r\n",
        "        return df\r\n",
        "\r\n",
        "class Processer(object):\r\n",
        "    @staticmethod\r\n",
        "    def drop_columns(df, col_names):\r\n",
        "        print('Before drop columns {0}'.format(df.shape))\r\n",
        "        df = df.drop(col_names, axis=1)\r\n",
        "        print('After drop columns {0}'.format(df.shape))\r\n",
        "        return df\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def dtype_transform(df):\r\n",
        "        for col in df.select_dtypes(include=['float64']).columns:\r\n",
        "            df[col] = df[col].astype(np.float32)\r\n",
        "        for col in df.select_dtypes(include=['int64']).columns:\r\n",
        "            df[col] = df[col].astype(np.int8)\r\n",
        "        return df\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def negative_one_vals(df):\r\n",
        "        df['negative_one_vals'] = MinMaxScaler().fit_transform(df.isnull().sum(axis=1).values.reshape(-1,1))\r\n",
        "        return df\r\n",
        "\r\n",
        "    @staticmethod\r\n",
        "    def ohe(df_train, df_test, cat_features, threshold=50):\r\n",
        "        # Train & test should get_dummies together\r\n",
        "        print('Before ohe : train {0}, test {1}'.format(df_train.shape, df_test.shape))\r\n",
        "        combine = pd.concat([df_train, df_test], axis=0)\r\n",
        "        for column in cat_features:\r\n",
        "            temp = pd.get_dummies(pd.Series(combine[column]), prefix=column)\r\n",
        "            _abort_cols = []\r\n",
        "            for c in temp.columns:\r\n",
        "                if temp[c].sum() < threshold:\r\n",
        "                    print('column {0} unique value {1} less than threshold {2}'.format(c, temp[c].sum(), threshold))\r\n",
        "                    _abort_cols.append(c)\r\n",
        "            print('Abort cat columns : {0}'.format(_abort_cols))\r\n",
        "            _remain_cols = [ c for c in temp.columns if c not in _abort_cols ]\r\n",
        "            # check category number\r\n",
        "            combine = pd.concat([combine, temp[_remain_cols]], axis=1)\r\n",
        "            combine = combine.drop([column], axis=1)\r\n",
        "        train = combine[:df_train.shape[0]]\r\n",
        "        test = combine[df_train.shape[0]:]\r\n",
        "        print('After ohe : train {0}, test {1}'.format(train.shape, test.shape))\r\n",
        "        return train, test\r\n",
        "\r\n",
        "\r\n",
        "Number_of_folds = 5\r\n",
        "comm_skf = StratifiedKFold(n_splits=Number_of_folds, shuffle=True, random_state=2017)\r\n",
        "\r\n",
        "\r\n",
        "df_train, df_y, df_test, df_sub, train_id = read_data()\r\n",
        "skf = comm_skf\r\n",
        "\r\n",
        "## Processing and Feature Engineering\r\n",
        "transformer_one = [\r\n",
        "    (Processer.drop_columns, dict(col_names=df_train.columns[df_train.columns.str.startswith('ps_calc_')])),\r\n",
        "    (Processer.drop_columns, dict(col_names=['ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin'])),\r\n",
        "    (Processer.negative_one_vals, dict()),\r\n",
        "    (Processer.dtype_transform, dict()),\r\n",
        "]\r\n",
        "# Executing transform pipeline\r\n",
        "print('Transform train data')\r\n",
        "df_train = Compose(transformer_one)(df_train)\r\n",
        "print('Transform test data')\r\n",
        "df_test = Compose(transformer_one)(df_test)\r\n",
        "# Executing ohe\r\n",
        "df_train, df_test = Processer.ohe(df_train, df_test, [a for a in df_train.columns if a.endswith('cat')])\r\n",
        "\r\n",
        "# Extracting feature and label for train\r\n",
        "X = df_train.values\r\n",
        "y = df_y.values\r\n",
        "\r\n",
        "gc.collect()\r\n",
        "\r\n",
        "## cv and oof\r\n",
        "params_for_submit = {\r\n",
        "    'objective': 'binary:logistic',\r\n",
        "    'eval_metric': 'logloss',\r\n",
        "    'eta': 0.04,\r\n",
        "    'max_depth': 5,\r\n",
        "    'min_child_weight': 9.15,\r\n",
        "    'gamma': 0.59,\r\n",
        "    'subsample': 0.8,\r\n",
        "    'colsample_bytree': 0.8,\r\n",
        "    'alpha': 10.4,\r\n",
        "    'lambda': 5,\r\n",
        "    'seed': 2017,\r\n",
        "    'nthread': 5,\r\n",
        "    'silent': 1,\r\n",
        "}\r\n",
        "single_xgb = SingleXGB(X=X, y=y, test=df_test, skf=skf, N=Number_of_folds)\r\n",
        "best_rounds = 431\r\n",
        "df_sub, stacker_train = single_xgb.oof(\r\n",
        "    params=params_for_submit,\r\n",
        "    best_rounds=best_rounds,\r\n",
        "    sub=df_sub,\r\n",
        "    do_logit=False\r\n",
        ")\r\n",
        "# writing submit file and training file to local disk\r\n",
        "write_data(\r\n",
        "    df_sub=df_sub,\r\n",
        "    train_id=train_id,\r\n",
        "    stacker_train=stacker_train,\r\n",
        "    sub_filename='sub_single_xgb_001_test.csv',\r\n",
        "    train_filename='single_xgb_001_train.csv'\r\n",
        ")\r\n",
        "print('Single XGBoost done')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "six9dTZ9DTwd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-7kGiTV_F_y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_Q1HnEqz7q-"
      },
      "source": [
        "#### Programmatic case 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wY06qodhz6pQ"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from multiprocessing import *\r\n",
        "import gc\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "import xgboost as xgb\r\n",
        "\r\n",
        "train = pd.read_csv('../content/train.csv')\r\n",
        "test = pd.read_csv('../content/test.csv')\r\n",
        "\r\n",
        "### \r\n",
        "y = train['target'].values\r\n",
        "testid= test['id'].values\r\n",
        "\r\n",
        "train.drop(['id','target'],axis=1,inplace=True)\r\n",
        "test.drop(['id'],axis=1,inplace=True)\r\n",
        "\r\n",
        "### Drop calc\r\n",
        "unwanted = train.columns[train.columns.str.startswith('ps_calc_')]\r\n",
        "train = train.drop(unwanted, axis=1)  \r\n",
        "test = test.drop(unwanted, axis=1)\r\n",
        "\r\n",
        "\r\n",
        "def recon(reg):\r\n",
        "    integer = int(np.round((40*reg)**2)) \r\n",
        "    for a in range(32):\r\n",
        "        if (integer - a) % 31 == 0:\r\n",
        "            A = a\r\n",
        "    M = (integer - A)//31\r\n",
        "    return A, M\r\n",
        "train['ps_reg_A'] = train['ps_reg_03'].apply(lambda x: recon(x)[0])\r\n",
        "train['ps_reg_M'] = train['ps_reg_03'].apply(lambda x: recon(x)[1])\r\n",
        "train['ps_reg_A'].replace(19,-1, inplace=True)\r\n",
        "train['ps_reg_M'].replace(51,-1, inplace=True)\r\n",
        "test['ps_reg_A'] = test['ps_reg_03'].apply(lambda x: recon(x)[0])\r\n",
        "test['ps_reg_M'] = test['ps_reg_03'].apply(lambda x: recon(x)[1])\r\n",
        "test['ps_reg_A'].replace(19,-1, inplace=True)\r\n",
        "test['ps_reg_M'].replace(51,-1, inplace=True)\r\n",
        "\r\n",
        "\r\n",
        "### Froza's baseline\r\n",
        "\r\n",
        "d_median = train.median(axis=0)\r\n",
        "d_mean = train.mean(axis=0)\r\n",
        "d_skew = train.skew(axis=0)\r\n",
        "one_hot = {c: list(train[c].unique()) for c in train.columns if c not in ['id','target']}\r\n",
        "\r\n",
        "def transform_df(df):\r\n",
        "    df = pd.DataFrame(df)\r\n",
        "    dcol = [c for c in df.columns if c not in ['id','target']]\r\n",
        "    df['ps_car_13_x_ps_reg_03'] = df['ps_car_13'] * df['ps_reg_03']\r\n",
        "    df['negative_one_vals'] = np.sum((df[dcol]==-1).values, axis=1)\r\n",
        "    for c in dcol:\r\n",
        "        if '_bin' not in c: #standard arithmetic\r\n",
        "            df[c+str('_median_range')] = (df[c].values > d_median[c]).astype(np.int)\r\n",
        "            df[c+str('_mean_range')] = (df[c].values > d_mean[c]).astype(np.int)\r\n",
        "\r\n",
        "    for c in one_hot:\r\n",
        "        if len(one_hot[c])>2 and len(one_hot[c]) < 7:\r\n",
        "            for val in one_hot[c]:\r\n",
        "                df[c+'_oh_' + str(val)] = (df[c].values == val).astype(np.int)\r\n",
        "    return df\r\n",
        "\r\n",
        "def multi_transform(df):\r\n",
        "    print('Init Shape: ', df.shape)\r\n",
        "    p = Pool(cpu_count())\r\n",
        "    df = p.map(transform_df, np.array_split(df, cpu_count()))\r\n",
        "    df = pd.concat(df, axis=0, ignore_index=True).reset_index(drop=True)\r\n",
        "    p.close(); p.join()\r\n",
        "    print('After Shape: ', df.shape)\r\n",
        "    return df\r\n",
        "\r\n",
        "train = multi_transform(train)\r\n",
        "test = multi_transform(test)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "### Gini\r\n",
        "\r\n",
        "def ginic(actual, pred):\r\n",
        "    actual = np.asarray(actual) \r\n",
        "    n = len(actual)\r\n",
        "    a_s = actual[np.argsort(pred)]\r\n",
        "    a_c = a_s.cumsum()\r\n",
        "    giniSum = a_c.sum() / a_s.sum() - (n + 1) / 2.0\r\n",
        "    return giniSum / n\r\n",
        " \r\n",
        "def gini_normalized(a, p):\r\n",
        "    if p.ndim == 2:\r\n",
        "        p = p[:,1] \r\n",
        "    return ginic(a, p) / ginic(a, a)\r\n",
        "    \r\n",
        "\r\n",
        "def gini_xgb(preds, dtrain):\r\n",
        "    labels = dtrain.get_label()\r\n",
        "    gini_score = gini_normalized(labels, preds)\r\n",
        "    return 'gini', gini_score\r\n",
        "\r\n",
        "\r\n",
        "### XGB modeling\r\n",
        "\r\n",
        "sub = pd.DataFrame()\r\n",
        "sub['id'] = testid\r\n",
        "params = {'eta': 0.025, 'max_depth': 4, \r\n",
        "          'subsample': 0.9, 'colsample_bytree': 0.7, \r\n",
        "          'colsample_bylevel':0.7,\r\n",
        "            'min_child_weight':100,\r\n",
        "            'alpha':4,\r\n",
        "            'objective': 'binary:logistic', 'eval_metric': 'auc', 'seed': 99, 'silent': True}\r\n",
        "x1, x2, y1, y2 = train_test_split(train, y, test_size=0.25, random_state=99)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "watchlist = [(xgb.DMatrix(x1, y1), 'train'), (xgb.DMatrix(x2, y2), 'valid')]\r\n",
        "model = xgb.train(params, xgb.DMatrix(x1, y1), 5000,  watchlist, feval=gini_xgb, maximize=True, \r\n",
        "                  verbose_eval=100, early_stopping_rounds=70)\r\n",
        "sub['target'] = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit)\r\n",
        "\r\n",
        "### Submission\r\n",
        "\r\n",
        "sub['target'] = model.predict(xgb.DMatrix(test), ntree_limit=model.best_ntree_limit)\r\n",
        "sub.to_csv('Result.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}