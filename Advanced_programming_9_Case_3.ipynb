{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Advanced_programming_9_Case_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GYDC0IX8-mf"
      },
      "source": [
        "## Confidentiality\r\n",
        "\r\n",
        "The programmatic cases in this notebook are utilized from different internet resources (in this notebook especially from kaggle.com) and are for demonstrational purposes only.\r\n",
        "\r\n",
        "Please do not copy or distribute this notebook.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1VoV6ir9EEY"
      },
      "source": [
        "## Table of content\r\n",
        "\r\n",
        "Santander Bank Customer Transaction\r\n",
        "\r\n",
        "1. Programmatic case 1 \r\n",
        "2. Programmatic case 2\r\n",
        "3. Programmatic case 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_SB6PLi9HSj"
      },
      "source": [
        "## Previous knowledge\r\n",
        "\r\n",
        "For a good understanding of this notebook you should have a few years of data-science and programming experience and have studied the advanced programming notebooks.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66-RuzWk37cq"
      },
      "source": [
        "#### Programmatic case 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yickaOP6YIe5"
      },
      "source": [
        "import fastai\r\n",
        "from fastai.tabular import *\r\n",
        "from fastai.text import *\r\n",
        "import feather\r\n",
        "import gc\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "from tqdm import tqdm\r\n",
        "from fastai.callbacks import SaveModelCallback\r\n",
        "import logging\r\n",
        "\r\n",
        "#logger\r\n",
        "def get_logger():\r\n",
        "    FORMAT = '[%(levelname)s]%(asctime)s:%(name)s:%(message)s'\r\n",
        "    logging.basicConfig(format=FORMAT)\r\n",
        "    logger = logging.getLogger('main')\r\n",
        "    logger.setLevel(logging.DEBUG)\r\n",
        "    return logger\r\n",
        "    \r\n",
        "logger = get_logger()\r\n",
        "\r\n",
        "def auroc_score(input, target):\r\n",
        "    input, target = input.cpu().numpy()[:,1], target.cpu().numpy()\r\n",
        "    return roc_auc_score(target, input)\r\n",
        "\r\n",
        "# Callback to calculate AUC at the end of each epoch\r\n",
        "class AUROC(Callback):\r\n",
        "    _order = -20 #Needs to run before the recorder\r\n",
        "\r\n",
        "    def __init__(self, learn, **kwargs): self.learn = learn\r\n",
        "    def on_train_begin(self, **kwargs): self.learn.recorder.add_metric_names(['AUROC'])\r\n",
        "    def on_epoch_begin(self, **kwargs): self.output, self.target = [], []\r\n",
        "    \r\n",
        "    def on_batch_end(self, last_target, last_output, train, **kwargs):\r\n",
        "        if not train:\r\n",
        "            self.output.append(last_output)\r\n",
        "            self.target.append(last_target)\r\n",
        "                \r\n",
        "    def on_epoch_end(self, last_metrics, **kwargs):\r\n",
        "        if len(self.output) > 0:\r\n",
        "            output = torch.cat(self.output)\r\n",
        "            target = torch.cat(self.target)\r\n",
        "            preds = F.softmax(output, dim=1)\r\n",
        "            metric = auroc_score(preds, target)\r\n",
        "            return add_metrics(last_metrics, [metric])\r\n",
        "\r\n",
        "# Callback that do the shuffle augmentation        \r\n",
        "class AugShuffCallback(LearnerCallback):\r\n",
        "    def __init__(self, learn:Learner):\r\n",
        "        super().__init__(learn)\r\n",
        "        \r\n",
        "    def on_batch_begin(self, last_input, last_target, train, **kwargs):\r\n",
        "        if not train: return\r\n",
        "        m_pos = last_target==1\r\n",
        "        m_neg = last_target==0\r\n",
        "        \r\n",
        "        pos_cat = last_input[0][m_pos]\r\n",
        "        pos_cont = last_input[1][m_pos]\r\n",
        "        \r\n",
        "        neg_cat = last_input[0][m_neg]\r\n",
        "        neg_cont = last_input[1][m_neg]\r\n",
        "        \r\n",
        "        for f in range(200):\r\n",
        "            shuffle_pos = torch.randperm(pos_cat.size(0)).to(last_input[0].device)\r\n",
        "            pos_cat[:,f] = pos_cat[shuffle_pos,f]\r\n",
        "            pos_cont[:,f] = pos_cont[shuffle_pos, f]\r\n",
        "            pos_cont[:,f+200] = pos_cont[shuffle_pos, f+200]\r\n",
        "            \r\n",
        "            shuffle_neg = torch.randperm(neg_cat.size(0)).to(last_input[0].device)\r\n",
        "            neg_cat[:,f] = neg_cat[shuffle_neg,f]\r\n",
        "            neg_cont[:, f] = neg_cont[shuffle_neg, f]\r\n",
        "            neg_cont[:,f+200] = neg_cont[shuffle_neg, f+200]\r\n",
        "        \r\n",
        "        new_input = [torch.cat([pos_cat, neg_cat]), torch.cat([pos_cont, neg_cont])]\r\n",
        "        new_target = torch.cat([last_target[m_pos], last_target[m_neg]])\r\n",
        "        \r\n",
        "        return {'last_input': new_input, 'last_target': new_target}\r\n",
        "        \r\n",
        "# Just a longer version of the random sampler : each samples is given \"mult\" times.\r\n",
        "class LongerRandomSampler(Sampler):\r\n",
        "    def __init__(self, data_source, replacement=False, num_samples=None, mult=3):\r\n",
        "        self.data_source = data_source\r\n",
        "        self.replacement = replacement\r\n",
        "        self.num_samples = num_samples\r\n",
        "        self.mult = mult\r\n",
        "\r\n",
        "        if self.num_samples is not None and replacement is False:\r\n",
        "            raise ValueError(\"With replacement=False, num_samples should not be specified, \"\r\n",
        "                             \"since a random permute will be performed.\")\r\n",
        "\r\n",
        "        if self.num_samples is None:\r\n",
        "            self.num_samples = len(self.data_source) * self.mult\r\n",
        "\r\n",
        "        if not isinstance(self.num_samples, int) or self.num_samples <= 0:\r\n",
        "            raise ValueError(\"num_samples should be a positive integeral \"\r\n",
        "                             \"value, but got num_samples={}\".format(self.num_samples))\r\n",
        "        if not isinstance(self.replacement, bool):\r\n",
        "            raise ValueError(\"replacement should be a boolean value, but got \"\r\n",
        "                             \"replacement={}\".format(self.replacement))\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        n = len(self.data_source)\r\n",
        "        if self.replacement:\r\n",
        "            return iter(torch.randint(high=n, size=(self.num_samples*self.mult,), dtype=torch.int64).tolist())\r\n",
        "        return iter(torch.randperm(n).tolist()*self.mult)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data_source)*self.mult\r\n",
        "        \r\n",
        "# This is the NN structure, starting from fast.ai TabularModel.\r\n",
        "class my_TabularModel(nn.Module):\r\n",
        "    \"Basic model for tabular data.\"\r\n",
        "    def __init__(self, emb_szs:ListSizes, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None,\r\n",
        "                 emb_drop:float=0., y_range:OptRange=None, use_bn:bool=True, bn_final:bool=False, \r\n",
        "                 cont_emb=2, cont_emb_notu=2):\r\n",
        "        \r\n",
        "        super().__init__()\r\n",
        "        # \"Continuous embedding NN for raw features\"\r\n",
        "        self.cont_emb = cont_emb[1]\r\n",
        "        self.cont_emb_l = torch.nn.Linear(1 + 2, cont_emb[0])\r\n",
        "        self.cont_emb_l2 = torch.nn.Linear(cont_emb[0], cont_emb[1])\r\n",
        "        \r\n",
        "        # \"Continuous embedding NN for \"not unique\" features\". cf #1 solution post\r\n",
        "        self.cont_emb_notu_l = torch.nn.Linear(1 + 2, cont_emb_notu[0])\r\n",
        "        self.cont_emb_notu_l2 = torch.nn.Linear(cont_emb_notu[0], cont_emb_notu[1])\r\n",
        "        self.cont_emb_notu = cont_emb_notu[1]\r\n",
        "            \r\n",
        "        ps = ifnone(ps, [0]*len(layers))\r\n",
        "        ps = listify(ps, layers)\r\n",
        "        \r\n",
        "        # Embedding for \"has one\" categorical features, cf #1 solution post\r\n",
        "        self.embeds = embedding(emb_szs[0][0], emb_szs[0][1])\r\n",
        "        \r\n",
        "        # At first information was included about the variable being processed (to extract feature importance). \r\n",
        "        # It works better using a constant feat (kind of intercept)\r\n",
        "        self.embeds_feat = embedding(201, 2)\r\n",
        "        self.embeds_feat_w = embedding(201, 2)\r\n",
        "        \r\n",
        "        self.emb_drop = nn.Dropout(emb_drop)\r\n",
        "        \r\n",
        "        n_emb = self.embeds.embedding_dim\r\n",
        "        n_emb_feat = self.embeds_feat.embedding_dim\r\n",
        "        n_emb_feat_w = self.embeds_feat_w.embedding_dim\r\n",
        "        \r\n",
        "        self.n_emb, self.n_emb_feat, self.n_emb_feat_w, self.n_cont,self.y_range = n_emb, n_emb_feat, n_emb_feat_w, n_cont, y_range\r\n",
        "        \r\n",
        "        sizes = self.get_sizes(layers, out_sz)\r\n",
        "        actns = [nn.ReLU(inplace=True)] * (len(sizes)-2) + [None]\r\n",
        "        layers = []\r\n",
        "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\r\n",
        "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\r\n",
        "            \r\n",
        "        self.layers = nn.Sequential(*layers)\r\n",
        "        self.seq = nn.Sequential()\r\n",
        "        \r\n",
        "        # Input size for the NN that predicts weights\r\n",
        "        inp_w = self.n_emb + self.n_emb_feat_w + self.cont_emb + self.cont_emb_notu\r\n",
        "        # Input size for the final NN that predicts output\r\n",
        "        inp_x = self.n_emb + self.cont_emb + self.cont_emb_notu\r\n",
        "        \r\n",
        "        # NN that predicts the weights\r\n",
        "        self.weight = nn.Linear(inp_w, 5)\r\n",
        "        self.weight2 = nn.Linear(5,1)\r\n",
        "        \r\n",
        "        mom = 0.1\r\n",
        "        self.bn_cat = nn.BatchNorm1d(200, momentum=mom)\r\n",
        "        self.bn_feat_emb = nn.BatchNorm1d(200, momentum=mom)\r\n",
        "        self.bn_feat_w = nn.BatchNorm1d(200, momentum=mom)\r\n",
        "        self.bn_raw = nn.BatchNorm1d(200, momentum=mom)\r\n",
        "        self.bn_notu = nn.BatchNorm1d(200, momentum=mom)\r\n",
        "        self.bn_w = nn.BatchNorm1d(inp_w, momentum=mom)\r\n",
        "        self.bn = nn.BatchNorm1d(inp_x, momentum=mom)\r\n",
        "        \r\n",
        "    def get_sizes(self, layers, out_sz):\r\n",
        "        return [self.n_emb + self.cont_emb_notu + self.cont_emb] + layers + [out_sz]\r\n",
        "\r\n",
        "    def forward(self, x_cat:Tensor, x_cont:Tensor) -> Tensor:\r\n",
        "        b_size = x_cont.size(0)\r\n",
        "        \r\n",
        "        # embedding of has one feat\r\n",
        "        x = [self.embeds(x_cat[:,i]) for i in range(200)]\r\n",
        "        x = torch.stack(x, dim=1)\r\n",
        "        \r\n",
        "        # embedding of intercept. It was embedding of feature id before\r\n",
        "        x_feat_emb = self.embeds_feat(x_cat[:,200])\r\n",
        "        x_feat_emb = torch.stack([x_feat_emb]*200, 1)\r\n",
        "        x_feat_emb = self.bn_feat_emb(x_feat_emb)\r\n",
        "        x_feat_w = self.embeds_feat_w(x_cat[:,200])\r\n",
        "        x_feat_w = torch.stack([x_feat_w]*200, 1)\r\n",
        "        \r\n",
        "        # \"continuous embedding\" of raw features\r\n",
        "        x_cont_raw = x_cont[:,:200].contiguous().view(-1, 1)\r\n",
        "        x_cont_raw = torch.cat([x_cont_raw, x_feat_emb.view(-1, self.n_emb_feat)], 1)\r\n",
        "        x_cont_raw = F.relu(self.cont_emb_l(x_cont_raw))\r\n",
        "        x_cont_raw = self.cont_emb_l2(x_cont_raw)\r\n",
        "        x_cont_raw = x_cont_raw.view(b_size, 200, self.cont_emb)\r\n",
        "        \r\n",
        "        # \"continuous embedding\" of not unique features\r\n",
        "        x_cont_notu = x_cont[:,200:].contiguous().view(-1, 1)\r\n",
        "        x_cont_notu = torch.cat([x_cont_notu, x_feat_emb.view(-1,self.n_emb_feat)], 1)\r\n",
        "        x_cont_notu = F.relu(self.cont_emb_notu_l(x_cont_notu))\r\n",
        "        x_cont_notu = self.cont_emb_notu_l2(x_cont_notu)\r\n",
        "        x_cont_notu = x_cont_notu.view(b_size, 200, self.cont_emb_notu)\r\n",
        "\r\n",
        "        x_cont_notu = self.bn_notu(x_cont_notu)\r\n",
        "        x = self.bn_cat(x)\r\n",
        "        x_cont_raw = self.bn_raw(x_cont_raw)\r\n",
        "\r\n",
        "        x = self.emb_drop(x)\r\n",
        "        x_cont_raw = self.emb_drop(x_cont_raw)\r\n",
        "        x_cont_notu = self.emb_drop(x_cont_notu)\r\n",
        "        x_feat_w = self.bn_feat_w(x_feat_w)\r\n",
        "        \r\n",
        "        # Predict a weight for each of the previous embeddings\r\n",
        "        x_w = torch.cat([x.view(-1,self.n_emb),\r\n",
        "                         x_feat_w.view(-1,self.n_emb_feat_w),\r\n",
        "                         x_cont_raw.view(-1, self.cont_emb), \r\n",
        "                         x_cont_notu.view(-1, self.cont_emb_notu)], 1)\r\n",
        "\r\n",
        "        x_w = self.bn_w(x_w)\r\n",
        "\r\n",
        "        w = F.relu(self.weight(x_w))\r\n",
        "        w = self.weight2(w).view(b_size, -1)\r\n",
        "        w = torch.nn.functional.softmax(w, dim=-1).unsqueeze(-1)\r\n",
        "\r\n",
        "        # weighted average of the differents embeddings using weights given by NN\r\n",
        "        x = (w * x).sum(dim=1)\r\n",
        "        x_cont_raw = (w * x_cont_raw).sum(dim=1)\r\n",
        "        x_cont_notu = (w * x_cont_notu).sum(dim=1)\r\n",
        "        \r\n",
        "        # Use NN on the weighted average to predict final output\r\n",
        "        x = torch.cat([x, x_cont_raw, x_cont_notu], 1) if self.n_emb != 0 else x_cont\r\n",
        "        x = self.bn(x)\r\n",
        "            \r\n",
        "        x = self.seq(x)\r\n",
        "        x = self.layers(x)\r\n",
        "        return x\r\n",
        "    \r\n",
        "def set_seed(seed=42):\r\n",
        "    # python RNG\r\n",
        "    random.seed(seed)\r\n",
        "\r\n",
        "    # pytorch RNGs\r\n",
        "    import torch\r\n",
        "    torch.manual_seed(seed)\r\n",
        "    torch.backends.cudnn.deterministic = True\r\n",
        "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\r\n",
        "\r\n",
        "    # numpy RNG\r\n",
        "    import numpy as np\r\n",
        "    np.random.seed(seed)\r\n",
        "    \r\n",
        "ss = StandardScaler()\r\n",
        "\r\n",
        "logger.info('Input data')\r\n",
        "\r\n",
        "data = pd.read_csv('../content/train.csv')\r\n",
        "data = data.set_index('ID_code')\r\n",
        "\r\n",
        "etd = pd.read_csv('../content/test.csv')\r\n",
        "etd = etd.set_index('ID_code')\r\n",
        "\r\n",
        "has_one = [f'var_{i}_has_one' for i in range(200)]\r\n",
        "orig = [f'var_{i}' for i in range(200)]\r\n",
        "not_u = [f'var_{i}_not_unique' for i in range(200)]\r\n",
        "\r\n",
        "cont_vars = orig + not_u\r\n",
        "cat_vars = has_one\r\n",
        "target = 'target'\r\n",
        "path = './'\r\n",
        "\r\n",
        "logger.info('cat treatment')\r\n",
        "\r\n",
        "for f in cat_vars:\r\n",
        "    data[f] = data[f].astype('category').cat.as_ordered()\r\n",
        "    etd[f] = pd.Categorical(etd[f], categories=data[f].cat.categories, ordered=True)\r\n",
        "\r\n",
        "# constant feature to replace feature index information\r\n",
        "feat = ['intercept']\r\n",
        "data['intercept'] = 1\r\n",
        "data['intercept'] = data['intercept'].astype('category')\r\n",
        "etd['intercept'] = 1\r\n",
        "etd['intercept'] = etd['intercept'].astype('category')\r\n",
        "    \r\n",
        "cat_vars += feat\r\n",
        "\r\n",
        "ref = pd.concat([data[cont_vars + cat_vars + ['target']], etd[cont_vars + cat_vars]])\r\n",
        "ref[cont_vars] = ss.fit_transform(ref[cont_vars].values)\r\n",
        "\r\n",
        "data = ref.iloc[:200000]\r\n",
        "etd = ref.iloc[200000:]\r\n",
        "\r\n",
        "data[target] = data[target].astype('int')\r\n",
        "\r\n",
        "del ref; gc.collect()\r\n",
        "\r\n",
        "fold_seed = 42\r\n",
        "ss = StratifiedKFold(n_splits=10, random_state=fold_seed, shuffle=True)\r\n",
        "\r\n",
        "folds = []\r\n",
        "for num, (train,test) in enumerate(ss.split(data[target], data[target])):\r\n",
        "    folds.append([train, test])\r\n",
        "\r\n",
        "\r\n",
        "layers=[32]\r\n",
        "ps=0.2\r\n",
        "emb_drop=0.08\r\n",
        "cont_emb=(50,10)\r\n",
        "cont_emb_notu=(50,10)\r\n",
        "emb_szs = [[6,12]]\r\n",
        "use_bn = True\r\n",
        "joined=False\r\n",
        "# Code modified to sub with one seed\r\n",
        "seeds = [42] #, 1337, 666]\r\n",
        "\r\n",
        "results = []\r\n",
        "sub_preds = pd.DataFrame(columns=range(10), index=etd.index)\r\n",
        "for num_fold, (train, test) in enumerate(folds):\r\n",
        "    procs=[]\r\n",
        "    df = (TabularList.from_df(data, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs)\r\n",
        "                .split_by_idx(test)\r\n",
        "                .label_from_df(cols=target)\r\n",
        "            .add_test(TabularList.from_df(etd, path=path, cat_names=cat_vars, cont_names=cont_vars, procs=procs))\r\n",
        "            .databunch(num_workers=0, bs=1024))\r\n",
        "            \r\n",
        "    df.dls[0].dl = df.dls[0].new(sampler=LongerRandomSampler(data_source=df.train_ds, mult=2), shuffle=False).dl\r\n",
        "    for num_seed, seed in enumerate(seeds):\r\n",
        "        logger.info(f'Model {num_fold} seed {num_seed}')\r\n",
        "        set_seed(seed)\r\n",
        "        model = my_TabularModel(emb_szs, len(df.cont_names), out_sz=df.c, layers=layers, ps=ps, emb_drop=emb_drop,\r\n",
        "                                 y_range=None, use_bn=use_bn, cont_emb=cont_emb, cont_emb_notu=cont_emb_notu)\r\n",
        "\r\n",
        "        learn = Learner(df, model, metrics=None, callback_fns=AUROC, wd=0.1)\r\n",
        "        learn.fit_one_cycle(15, max_lr=1e-2, callbacks=[SaveModelCallback(learn, every='improvement', monitor='AUROC', name=f'fold{fold_seed}_{num_fold}_seed_{seed}'), AugShuffCallback(learn)])\r\n",
        "        pred, _ = learn.get_preds()\r\n",
        "        pred = pred[:,1]\r\n",
        "        \r\n",
        "        pred_test, _ = learn.get_preds(DatasetType.Test)\r\n",
        "        pred_test = pred_test[:,1]\r\n",
        "        \r\n",
        "        sub_preds.loc[:, num_fold] = pred_test\r\n",
        "        results.append(np.max(learn.recorder.metrics))\r\n",
        "        logger.info('result ' + str(results[-1]))\r\n",
        "        \r\n",
        "        np.save(f'oof_fold{fold_seed}_{num_fold}_seed_{seed}.npy', pred)\r\n",
        "        np.save(f'test_fold{fold_seed}_{num_fold}_seed_{seed}.npy', pred_test)\r\n",
        "        \r\n",
        "        del learn, pred, model, pred_test; gc.collect()\r\n",
        "    del df; gc.collect()\r\n",
        "print(results)\r\n",
        "print(np.mean(results))\r\n",
        "\r\n",
        "sub_preds[target] = sub_preds.rank().mean(axis=1)\r\n",
        "sub_preds[[target]].to_csv('submission_NN_wo_pseudo_seed42.csv', index_label='ID_code')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHCp0ILyHmS6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IrzRmAcaTgi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BldKN8ma_Qkt"
      },
      "source": [
        "### Programmatic case 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCTrFuY9zdf_"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import lightgbm as lgb\r\n",
        "\r\n",
        "from sklearn.metrics import roc_auc_score, log_loss\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.model_selection import StratifiedKFold,KFold\r\n",
        "from scipy.stats import norm, skew\r\n",
        "\r\n",
        "from tqdm import tqdm_notebook as tqdm\r\n",
        "from copy import copy\r\n",
        "from multiprocessing import Pool\r\n",
        "\r\n",
        "from tqdm import tqdm_notebook as tqdm\r\n",
        "\r\n",
        "# Input data files are available in the \"../content/\" directory.\r\n",
        "import json, math, os, sys\r\n",
        "np.random.seed(42)\r\n",
        "# Any results you write to the current directory are saved as output.\r\n",
        "print(os.listdir(\"../content\"))\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "\r\n",
        "\r\n",
        "test_path = '/content/test.csv'\r\n",
        "\r\n",
        "df_test = pd.read_csv(test_path)\r\n",
        "df_test.drop(['ID_code'], axis=1, inplace=True)\r\n",
        "df_test = df_test.values\r\n",
        "\r\n",
        "unique_samples = []\r\n",
        "unique_count = np.zeros_like(df_test)\r\n",
        "for feature in tqdm(range(df_test.shape[1])):\r\n",
        "    _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\r\n",
        "    unique_count[index_[count_ == 1], feature] += 1\r\n",
        "\r\n",
        "# Samples which have unique values are good/real and others are bad/fake\r\n",
        "real_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\r\n",
        "synthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\r\n",
        "\r\n",
        "print(len(real_samples_indexes))\r\n",
        "print(len(synthetic_samples_indexes))\r\n",
        "\r\n",
        "\r\n",
        "df_test_real = df_test[real_samples_indexes].copy()\r\n",
        "\r\n",
        "generator_for_each_synthetic_sample = []\r\n",
        "# Using 20,000 samples should be enough. \r\n",
        "# You can use all of the 100,000 and get the same results (but 5 times slower)\r\n",
        "for cur_sample_index in tqdm(synthetic_samples_indexes[:20000]):\r\n",
        "    cur_synthetic_sample = df_test[cur_sample_index]\r\n",
        "    potential_generators = df_test_real == cur_synthetic_sample\r\n",
        "\r\n",
        "    # A verified generator for a synthetic sample is achieved\r\n",
        "    # only if the value of a feature appears only once in the\r\n",
        "    # entire real samples set\r\n",
        "    features_mask = np.sum(potential_generators, axis=0) == 1\r\n",
        "    verified_generators_mask = np.any(potential_generators[:, features_mask], axis=1)\r\n",
        "    verified_generators_for_sample = real_samples_indexes[np.argwhere(verified_generators_mask)[:, 0]]\r\n",
        "    generator_for_each_synthetic_sample.append(set(verified_generators_for_sample))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "public_LB = generator_for_each_synthetic_sample[0]\r\n",
        "for x in tqdm(generator_for_each_synthetic_sample):\r\n",
        "    if public_LB.intersection(x):\r\n",
        "        public_LB = public_LB.union(x)\r\n",
        "\r\n",
        "private_LB = generator_for_each_synthetic_sample[1]\r\n",
        "for x in tqdm(generator_for_each_synthetic_sample):\r\n",
        "    if private_LB.intersection(x):\r\n",
        "        private_LB = private_LB.union(x)\r\n",
        "        \r\n",
        "print(len(public_LB))\r\n",
        "print(len(private_LB))\r\n",
        "\r\n",
        "\r\n",
        "np.save('public_LB', list(public_LB))\r\n",
        "np.save('private_LB', list(private_LB))\r\n",
        "np.save('synthetic_samples_indexes', list(synthetic_samples_indexes))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "use_experimental = False\r\n",
        "\r\n",
        "train_df = pd.read_csv('../content/train.csv')\r\n",
        "test_df = pd.read_csv('../content/test.csv')\r\n",
        "\r\n",
        "indices_fake = np.load('../content/synthetic_samples_indexes.npy')\r\n",
        "indices_pub = np.load('../content/public_LB.npy')\r\n",
        "indices_pri = np.load('../content/private_LB.npy')\r\n",
        "indices_real = np.concatenate([indices_pub, indices_pri])\r\n",
        "\r\n",
        "features = [c for c in train_df.columns if c not in ['ID_code', 'target']]\r\n",
        "target_train = train_df['target']\r\n",
        "X_train = train_df\r\n",
        "X_test = test_df.loc[indices_real,:]\r\n",
        "X_test['target'] = np.zeros(X_test.shape[0])\r\n",
        "X_fake = test_df.loc[indices_fake,:]\r\n",
        "X_fake['target'] = np.zeros(X_test.shape[0])\r\n",
        "train_length = X_train.shape[0]\r\n",
        "target_test = X_test['target']\r\n",
        "target_fake = X_fake['target']\r\n",
        "\r\n",
        "if use_experimental:\r\n",
        "    np.random.seed(42)    \r\n",
        "    indices = np.arange(train_length)\r\n",
        "    train_length = 150000\r\n",
        "    np.random.shuffle(indices)\r\n",
        "    indices_train = indices[:train_length]\r\n",
        "    indices_test = indices[train_length:]\r\n",
        "    # Swapped order to not overwrite X_train to soon\r\n",
        "    X_test = X_train.iloc[indices_test,:]\r\n",
        "    X_fake = X_train.iloc[indices_test,:]\r\n",
        "    target_fake = X_fake['target']\r\n",
        "    X_train = X_train.iloc[indices_train,:]\r\n",
        "    target_train = X_train['target']\r\n",
        "    target_test = X_test['target']\r\n",
        "\r\n",
        "X_all = pd.concat([X_train, X_test])\r\n",
        "print(X_all.shape)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "import scipy.ndimage\r\n",
        "\r\n",
        "sigma_fac = 0.001\r\n",
        "sigma_base = 4\r\n",
        "\r\n",
        "eps = 0.00000001\r\n",
        "\r\n",
        "def get_count(X_all, X_fake):\r\n",
        "    features_count = np.zeros((X_all.shape[0], len(features)))\r\n",
        "    features_density = np.zeros((X_all.shape[0], len(features)))\r\n",
        "    features_deviation = np.zeros((X_all.shape[0], len(features)))\r\n",
        "\r\n",
        "    features_count_fake = np.zeros((X_fake.shape[0], len(features)))\r\n",
        "    features_density_fake = np.zeros((X_fake.shape[0], len(features)))\r\n",
        "    features_deviation_fake = np.zeros((X_fake.shape[0], len(features)))\r\n",
        "    \r\n",
        "    sigmas = []\r\n",
        "\r\n",
        "    for i,var in enumerate(tqdm(features)):\r\n",
        "        X_all_var_int = (X_all[var].values * 10000).round().astype(int)\r\n",
        "        X_fake_var_int = (X_fake[var].values * 10000).round().astype(int)\r\n",
        "        lo = X_all_var_int.min()\r\n",
        "        X_all_var_int -= lo\r\n",
        "        X_fake_var_int -= lo\r\n",
        "        hi = X_all_var_int.max()+1\r\n",
        "        counts_all = np.bincount(X_all_var_int, minlength=hi).astype(float)\r\n",
        "        zeros = (counts_all == 0).astype(int)\r\n",
        "        before_zeros = np.concatenate([zeros[1:],[0]])\r\n",
        "        indices_all = np.arange(counts_all.shape[0])\r\n",
        "        # Geometric mean of twice sigma_base and a sigma_scaled which is scaled to the length of array \r\n",
        "        sigma_scaled = counts_all.shape[0]*sigma_fac\r\n",
        "        sigma = np.power(sigma_base * sigma_base * sigma_scaled, 1/3)\r\n",
        "        sigmas.append(sigma)\r\n",
        "        counts_all_smooth = scipy.ndimage.filters.gaussian_filter1d(counts_all, sigma)\r\n",
        "        deviation = counts_all / (counts_all_smooth+eps)\r\n",
        "        indices = X_all_var_int\r\n",
        "        features_count[:,i] = counts_all[indices]\r\n",
        "        features_density[:,i] = counts_all_smooth[indices]\r\n",
        "        features_deviation[:,i] = deviation[indices]\r\n",
        "        indices_fake = X_fake_var_int\r\n",
        "        features_count_fake[:,i] = counts_all[indices_fake]\r\n",
        "        features_density_fake[:,i] = counts_all_smooth[indices_fake]\r\n",
        "        features_deviation_fake[:,i] = deviation[indices_fake]\r\n",
        "        \r\n",
        "    features_count_names = [var+'_count' for var in features]\r\n",
        "    features_density_names = [var+'_density' for var in features]\r\n",
        "    features_deviation_names = [var+'_deviation' for var in features]\r\n",
        "\r\n",
        "    X_all_count = pd.DataFrame(columns=features_count_names, data = features_count)\r\n",
        "    X_all_count.index = X_all.index\r\n",
        "    X_all_density = pd.DataFrame(columns=features_density_names, data = features_density)\r\n",
        "    X_all_density.index = X_all.index\r\n",
        "    X_all_deviation = pd.DataFrame(columns=features_deviation_names, data = features_deviation)\r\n",
        "    X_all_deviation.index = X_all.index\r\n",
        "    X_all = pd.concat([X_all,X_all_count, X_all_density, X_all_deviation], axis=1)\r\n",
        "    \r\n",
        "    X_fake_count = pd.DataFrame(columns=features_count_names, data = features_count_fake)\r\n",
        "    X_fake_count.index = X_fake.index\r\n",
        "    X_fake_density = pd.DataFrame(columns=features_density_names, data = features_density_fake)\r\n",
        "    X_fake_density.index = X_fake.index\r\n",
        "    X_fake_deviation = pd.DataFrame(columns=features_deviation_names, data = features_deviation_fake)\r\n",
        "    X_fake_deviation.index = X_fake.index\r\n",
        "    X_fake = pd.concat([X_fake,X_fake_count, X_fake_density, X_fake_deviation], axis=1)    \r\n",
        "\r\n",
        "    features_count = features_count_names\r\n",
        "    features_density = features_density_names\r\n",
        "    features_deviation = features_deviation_names\r\n",
        "    return X_all, features_count, features_density, features_deviation, X_fake\r\n",
        "\r\n",
        "X_all, features_count, features_density, features_deviation, X_fake = get_count(X_all, X_fake)\r\n",
        "print(X_all.shape)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "features_to_scale = [features, features_count]\r\n",
        "\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "\r\n",
        "def get_standardized(X_all, X_fake):\r\n",
        "    scaler = StandardScaler()\r\n",
        "    features_to_scale_flatten = [var for sublist in features_to_scale for var in sublist]\r\n",
        "    scaler.fit(X_all[features_to_scale_flatten])\r\n",
        "    features_scaled = scaler.transform(X_all[features_to_scale_flatten])\r\n",
        "    features_scaled_fake = scaler.transform(X_fake[features_to_scale_flatten])\r\n",
        "    X_all[features_to_scale_flatten] = features_scaled\r\n",
        "    X_fake[features_to_scale_flatten] = features_scaled_fake\r\n",
        "    return X_all, X_fake\r\n",
        "\r\n",
        "X_all, X_fake = get_standardized(X_all, X_fake)\r\n",
        "\r\n",
        "print(X_all.shape)\r\n",
        "\r\n",
        "\r\n",
        "X_train = X_all.iloc[:train_length,:]\r\n",
        "X_test = X_all.iloc[train_length:,:]\r\n",
        "del X_all\r\n",
        "import gc\r\n",
        "gc.collect()\r\n",
        "print(X_train.shape, X_test.shape)\r\n",
        "\r\n",
        "features_used = [features, features_count]\r\n",
        "\r\n",
        "\r\n",
        "params = {\r\n",
        "    'boost_from_average':'false',\r\n",
        "    'boost': 'gbdt',\r\n",
        "    'feature_fraction': 1,\r\n",
        "    'learning_rate': 0.08,\r\n",
        "    'max_depth': -1,\r\n",
        "    'metric':'binary_logloss',\r\n",
        "    'num_leaves': 4,\r\n",
        "    'num_threads': 8,\r\n",
        "    'tree_learner': 'serial',\r\n",
        "    'objective': 'binary',\r\n",
        "    'reg_alpha': 2,\r\n",
        "    'reg_lambda': 0,\r\n",
        "    'verbosity': 1,\r\n",
        "    'max_bin':256,\r\n",
        "}\r\n",
        "\r\n",
        "\r\n",
        "# reg_alpha\r\n",
        "reg_alpha_values = [0.75, 1, 2, 3]\r\n",
        "reg_alpha_var = [3, 0, 2, 3, 2, 0, 1, 1, 3, 2, 2, 0, 2, 0, 2, 2, 2, 1, 1, 2, \r\n",
        "                 1, 2, 3, 3, 2, 1, 3, 1, 3, 2, 2, 3, 1, 1, 3, 2, 0, 1, 0, 2, \r\n",
        "                 1, 1, 2, 3, 0, 3, 3, 3, 2, 0, 3, 1, 3, 1, 1, 0, 2, 2, 0, 0, \r\n",
        "                 0, 1, 2, 1, 0, 1, 3, 2, 0, 2, 1, 2, 0, 0, 1, 3, 3, 1, 2, 3, \r\n",
        "                 3, 2, 0, 1, 2, 3, 3, 2, 3, 3, 0, 0, 3, 0, 1, 0, 1, 0, 2, 3, \r\n",
        "                 1, 0, 3, 1, 3, 2, 3, 1, 3, 3, 3, 1, 3, 2, 3, 2, 1, 0, 1, 2, \r\n",
        "                 0, 3, 0, 3, 0, 3, 2, 1, 0, 0, 2, 2, 2, 0, 1, 0, 0, 2, 3, 2, \r\n",
        "                 2, 1, 1, 0, 1, 2, 2, 2, 1, 0, 2, 3, 2, 3, 1, 1, 3, 1, 1, 2, \r\n",
        "                 1, 2, 0, 3, 1, 3, 3, 2, 0, 1, 3, 3, 0, 1, 0, 3, 1, 3, 1, 3, \r\n",
        "                 0, 3, 0, 3, 1, 0, 0, 0, 3, 0, 3, 0, 0, 2, 0, 3, 1, 0, 3, 2]\r\n",
        "\r\n",
        "# max_bin\r\n",
        "max_bin_values = [256, 512, 1024]\r\n",
        "max_bin_var = [0, 0, 1, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, \r\n",
        "               2, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 1, 0, 0, 1, 0, \r\n",
        "               1, 1, 0, 1, 0, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 0, 1, 2, 1, 0, 0, \r\n",
        "               1, 2, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 2, 0, \r\n",
        "               1, 1, 0, 1, 0, 0, 1, 2, 1, 2, 1, 0, 0, 1, 0, 2, 0, 1, 0, 0, 2, \r\n",
        "               1, 1, 1, 0, 0, 0, 2, 0, 0, 2, 1, 0, 0, 1, 0, 1, 2, 0, 0, 0, 0, \r\n",
        "               0, 2, 2, 2, 2, 1, 1, 2, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 2, \r\n",
        "               0, 1, 0, 1, 1, 0, 2, 1, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, \r\n",
        "               0, 1, 0, 2, 0, 1, 0, 1, 2, 0, 0, 0, 0, 2, 0, 0, 2, 0, 1, 1, 0, \r\n",
        "               2, 0, 0, 0, 1, 2, 0, 0, 1, 0, 2]\r\n",
        "\r\n",
        "# learning_rate\r\n",
        "learning_rate_values = [0.06, 0.08, 0.12]\r\n",
        "learning_rate_var = [2, 2, 2, 1, 2, 2, 2, 0, 1, 2, 0, 2, 2, 2, 0, 2, 2, 0, 2, \r\n",
        "                     1, 2, 2, 2, 2, 2, 0, 1, 0, 2, 0, 0, 2, 0, 2, 2, 2, 1, 2, \r\n",
        "                     0, 0, 2, 0, 0, 1, 2, 1, 2, 0, 0, 2, 1, 2, 2, 2, 2, 0, 0, \r\n",
        "                     2, 1, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0, 2, 2, \r\n",
        "                     1, 0, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1, \r\n",
        "                     2, 0, 2, 0, 2, 0, 2, 1, 0, 0, 1, 2, 0, 2, 2, 2, 0, 2, 2, \r\n",
        "                     2, 2, 1, 0, 2, 1, 2, 2, 1, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, \r\n",
        "                     1, 2, 1, 0, 2, 1, 1, 2, 2, 2, 2, 0, 2, 1, 2, 1, 2, 2, 2, \r\n",
        "                     2, 2, 2, 2, 1, 1, 0, 1, 2, 0, 2, 2, 0, 1, 2, 2, 2, 1, 0, \r\n",
        "                     1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1, 0, 0, 2, 0, 1, 2, 1, 1, \r\n",
        "                     2, 2, 2, 2, 2, 2, 2, 2, 1, 1]\r\n",
        "\r\n",
        "# num_leaves\r\n",
        "num_leaves_values = [3, 4, 5]\r\n",
        "num_leaves_var = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, \r\n",
        "                  0, 1, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, \r\n",
        "                  0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, \r\n",
        "                  0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 1, 1, \r\n",
        "                  0, 0, 2, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, \r\n",
        "                  0, 1, 0, 1, 1, 2, 0, 0, 0, 0, 1, 0, 1, 2, 1, 1, 1, 0, 2, 0, 0, \r\n",
        "                  0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 1, 0, 2, \r\n",
        "                  2, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 2, 0, \r\n",
        "                  0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, \r\n",
        "                  0, 1, 2, 1, 1, 0, 0, 0, 2, 1, 2]\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "n_folds = 5\r\n",
        "early_stopping_rounds=10\r\n",
        "settings = [4]\r\n",
        "np.random.seed(47)\r\n",
        "\r\n",
        "settings_best_ind = []\r\n",
        "\r\n",
        "def train_trees():\r\n",
        "    preds_oof = np.zeros((len(X_train), len(features)))\r\n",
        "    preds_test = np.zeros((len(X_test), len(features)))\r\n",
        "    preds_train = np.zeros((len(X_train), len(features)))\r\n",
        "    preds_fake = np.zeros((len(X_fake), len(features)))\r\n",
        "\r\n",
        "    features_used_flatten = [var for sublist in features_used for var in sublist]\r\n",
        "    X_train_used = X_train[features_used_flatten]\r\n",
        "    X_test_used = X_test[features_used_flatten]\r\n",
        "    X_fake_used = X_fake[features_used_flatten]\r\n",
        "\r\n",
        "    for i in range(len(features)):\r\n",
        "        params['max_bin'] = max_bin_values[max_bin_var[i]]\r\n",
        "        params['learning_rate'] = learning_rate_values[learning_rate_var[i]]\r\n",
        "        params['reg_alpha'] = reg_alpha_values[reg_alpha_var[i]]\r\n",
        "        params['num_leaves'] = num_leaves_values[num_leaves_var[i]]\r\n",
        "        features_train = [feature_set[i] for feature_set in features_used] \r\n",
        "        print(f'Training on: {features_train}')\r\n",
        "        folds = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=np.random.randint(100000))\r\n",
        "        list_folds = list(folds.split(X_train_used.values, target_train.values))\r\n",
        "        preds_oof_temp = np.zeros((preds_oof.shape[0], len(settings)))\r\n",
        "        preds_test_temp = np.zeros((preds_test.shape[0], len(settings)))\r\n",
        "        preds_train_temp = np.zeros((preds_train.shape[0], len(settings)))\r\n",
        "        preds_fake_temp = np.zeros((preds_fake.shape[0], len(settings)))\r\n",
        "\r\n",
        "        scores = []\r\n",
        "        for j, setting in enumerate(settings):\r\n",
        "            # setting is used for hyperparameter tuning, here you can add sometinh like params['num_leaves'] = setting\r\n",
        "            print('\\nsetting: ', setting)\r\n",
        "            for k, (trn_idx, val_idx) in enumerate(list_folds):\r\n",
        "                print(\"Fold: {}\".format(k+1), end=\"\")\r\n",
        "                trn_data = lgb.Dataset(X_train_used.iloc[trn_idx][features_train], label=target_train.iloc[trn_idx])\r\n",
        "                val_data = lgb.Dataset(X_train_used.iloc[val_idx][features_train], label=target_train.iloc[val_idx])\r\n",
        "\r\n",
        "                # Binary Log Loss\r\n",
        "                clf = lgb.train(params, trn_data, 2000, valid_sets=[trn_data, val_data], verbose_eval=False, early_stopping_rounds=early_stopping_rounds) \r\n",
        "\r\n",
        "                prediction_val1 = clf.predict(X_train_used.iloc[val_idx][features_train])\r\n",
        "                prediction_test1 = clf.predict(X_test_used[features_train])\r\n",
        "                prediction_train1 = clf.predict(X_train_used.iloc[trn_idx][features_train])\r\n",
        "                prediction_fake1 = clf.predict(X_fake_used[features_train])\r\n",
        "\r\n",
        "                # Predictions\r\n",
        "                s1 = roc_auc_score(target_train.iloc[val_idx], prediction_val1)\r\n",
        "                s1_log = log_loss(target_train.iloc[val_idx], prediction_val1)\r\n",
        "                print(' - val AUC: {:<8.4f} - loss: {:<8.3f}'.format(s1, s1_log*1000), end='')\r\n",
        "\r\n",
        "                # Predictions Test\r\n",
        "                if use_experimental:\r\n",
        "                    s1_test = roc_auc_score(target_test, prediction_test1)\r\n",
        "                    s1_log_test = log_loss(target_test, prediction_test1)\r\n",
        "                    print(' - test AUC: {:<8.4f} - loss: {:<8.3f}'.format(s1_test, s1_log_test*1000), end='')\r\n",
        "\r\n",
        "                # Predictions Train\r\n",
        "                s1_train = roc_auc_score(target_train.iloc[trn_idx], prediction_train1)\r\n",
        "                s1_log_train = log_loss(target_train.iloc[trn_idx], prediction_train1)\r\n",
        "                print(' - train AUC: {:<8.4f} - loss: {:<8.3f}'.format(s1_train, s1_log_train*1000), end='')\r\n",
        "                if use_experimental:\r\n",
        "                    print('',clf.feature_importance(), end='')\r\n",
        "\r\n",
        "                print('')\r\n",
        "\r\n",
        "\r\n",
        "                preds_oof_temp[val_idx,j] += np.sqrt(prediction_val1 - prediction_val1.mean() + 0.1) \r\n",
        "                preds_test_temp[:,j] += np.sqrt(prediction_test1 - prediction_test1.mean() + 0.1) / n_folds\r\n",
        "                preds_train_temp[trn_idx,j] += np.sqrt(prediction_train1 - prediction_train1.mean() + 0.1) / (n_folds-1)\r\n",
        "                preds_fake_temp[:,j] += np.sqrt(prediction_fake1 - prediction_fake1.mean() + 0.1) / n_folds\r\n",
        "\r\n",
        "            score_setting = roc_auc_score(target_train, preds_oof_temp[:,j])\r\n",
        "            score_setting_log = 1000*log_loss(target_train, np.exp(preds_oof_temp[:,j]))\r\n",
        "            scores.append(score_setting_log)\r\n",
        "            print(\"Score:  - val AUC: {:<8.4f} - loss: {:<8.3f}\".format(score_setting, score_setting_log), end='')\r\n",
        "            if use_experimental:\r\n",
        "                score_setting_test = roc_auc_score(target_test, preds_test_temp[:,j])\r\n",
        "                score_setting_log_test = 1000*log_loss(target_test, np.exp(preds_test_temp[:,j]))  \r\n",
        "                print(\" - test AUC: {:<8.4f} - loss: {:<8.3f}\".format(score_setting_test, score_setting_log_test), end='')\r\n",
        "\r\n",
        "            score_setting_train = roc_auc_score(target_train, preds_train_temp[:,j])\r\n",
        "            score_setting_log_train = 1000*log_loss(target_train, np.exp(preds_train_temp[:,j]))\r\n",
        "            print(\" - train AUC: {:<8.4f} - loss: {:<8.3f}\".format(score_setting_train, score_setting_log_train))\r\n",
        "\r\n",
        "        best_ind = np.argmin(scores)\r\n",
        "        settings_best_ind.append(best_ind)\r\n",
        "        preds_oof[:,i] = preds_oof_temp[:,best_ind]\r\n",
        "        preds_test[:,i] = preds_test_temp[:,best_ind]\r\n",
        "        preds_train[:,i] = preds_train_temp[:,best_ind]\r\n",
        "        preds_fake[:,i] = preds_fake_temp[:,best_ind]\r\n",
        "\r\n",
        "\r\n",
        "        print('\\nbest setting: ', settings[best_ind])\r\n",
        "        preds_oof_cum = preds_oof[:,:i+1].mean(axis=1)\r\n",
        "        print(\"Cum CV val  : {:<8.4f} - loss: {:<8.3f}\".format(roc_auc_score(target_train, preds_oof_cum), 1000*log_loss(target_train, np.exp(preds_oof_cum))))\r\n",
        "        if use_experimental:        \r\n",
        "            preds_test_cum = preds_test[:,:i+1].mean(axis=1)\r\n",
        "            print(\"Cum CV test : {:<8.4f} - loss: {:<8.3f}\".format(roc_auc_score(target_test, preds_test_cum), 1000*log_loss(target_test, np.exp(preds_test_cum))))\r\n",
        "        preds_train_cum = preds_train[:,:i+1].mean(axis=1)\r\n",
        "        print(\"Cum CV train: {:<8.4f} - loss: {:<8.3f}\".format(roc_auc_score(target_train, preds_train_cum), 1000*log_loss(target_train, np.exp(preds_train_cum))))\r\n",
        "        print('*****' * 10 + '\\n')\r\n",
        "        \r\n",
        "    return preds_oof, preds_test, preds_train, preds_fake\r\n",
        "\r\n",
        "preds_oof, preds_test, preds_train, preds_fake = train_trees()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5ddi4AZzdl3"
      },
      "source": [
        "preds_oof_cum = np.zeros(preds_oof.shape[0])\r\n",
        "if use_experimental:\r\n",
        "    preds_test_cum = np.zeros(preds_test.shape[0])\r\n",
        "preds_train_cum = np.zeros(preds_train.shape[0])\r\n",
        "for i in range(len(features)):\r\n",
        "    preds_oof_cum += preds_oof[:,i]\r\n",
        "    preds_train_cum += preds_train[:,i]\r\n",
        "    print(\"var_{} Cum val: {:<8.5f}\".format(i,roc_auc_score(target_train, preds_oof_cum)), end=\"\")\r\n",
        "    if use_experimental:\r\n",
        "        preds_test_cum += preds_test[:,i]\r\n",
        "        print(\" - test : {:<8.5f}\".format(roc_auc_score(target_test, preds_test_cum)), end=\"\")\r\n",
        "    print(\" - train: {:<8.5f}\".format(roc_auc_score(target_train, preds_train_cum)))\r\n",
        "\r\n",
        "\r\n",
        "print(settings)\r\n",
        "print(settings_best_ind)\r\n",
        "\r\n",
        "\r\n",
        "from scipy.interpolate import interp1d\r\n",
        "from scipy.ndimage.filters import gaussian_filter\r\n",
        "import matplotlib.pyplot as plt \r\n",
        "\r\n",
        "features_to_show = np.arange(20)\r\n",
        "plt.figure(figsize = (20,20))\r\n",
        "\r\n",
        "for i in features_to_show:\r\n",
        "    var = 'var_'+str(i)\r\n",
        "    signal = X_test[var].values\r\n",
        "    logits = preds_test[:,i]\r\n",
        "    func = interp1d(signal, logits)\r\n",
        "    space = np.linspace(signal.min(), signal.max(), 4000)\r\n",
        "    activations = func(space)\r\n",
        "    activations_smooth = gaussian_filter(activations, 10)\r\n",
        "    \r\n",
        "    func_smooth = interp1d(space, activations_smooth)\r\n",
        "    logits_smooth = func_smooth(signal)\r\n",
        "    plt.subplot(5,4,i+1)\r\n",
        "    plt.plot(space, activations)\r\n",
        "    plt.plot(space, activations_smooth)\r\n",
        "\r\n",
        "\r\n",
        "import keras\r\n",
        "\r\n",
        "n_splits = 7\r\n",
        "num_preds = 5\r\n",
        "epochs = 60\r\n",
        "learning_rate_init = 0.02\r\n",
        "batch_size = 4000\r\n",
        "\r\n",
        "num_features = len(features)\r\n",
        "\r\n",
        "def get_features(preds, df):\r\n",
        "    list_features = [preds, df[features].values, df[features_count].values, df[features_deviation], df[features_density]]\r\n",
        "    list_indices = []\r\n",
        "    for i in range(num_features):\r\n",
        "        indices = np.arange(num_preds)*num_features + i\r\n",
        "        list_indices.append(indices)\r\n",
        "    indices = np.concatenate(list_indices)\r\n",
        "    feats = np.concatenate(list_features, axis=1)[:,indices]\r\n",
        "    return feats \r\n",
        "\r\n",
        "def get_model_3():\r\n",
        "    inp = keras.layers.Input((num_features*num_preds,))\r\n",
        "    x = keras.layers.Reshape((num_features*num_preds,1))(inp)\r\n",
        "    x = keras.layers.Conv1D(32,num_preds,strides=num_preds, activation='elu')(x)\r\n",
        "    x = keras.layers.BatchNormalization()(x)\r\n",
        "    x = keras.layers.Conv1D(24,1, activation='elu')(x)\r\n",
        "    x = keras.layers.BatchNormalization()(x)\r\n",
        "    x = keras.layers.Conv1D(16,1, activation='elu')(x)\r\n",
        "    x = keras.layers.BatchNormalization()(x)\r\n",
        "    x = keras.layers.Conv1D(4,1, activation='elu')(x)\r\n",
        "    x = keras.layers.Flatten()(x)\r\n",
        "    x = keras.layers.Reshape((num_features*4,1))(x)\r\n",
        "    x = keras.layers.AveragePooling1D(2)(x)\r\n",
        "    x = keras.layers.Flatten()(x)\r\n",
        "    x = keras.layers.BatchNormalization()(x)\r\n",
        "    out = keras.layers.Dense(1, activation='sigmoid')(x)\r\n",
        "    return keras.Model(inputs=inp, outputs=out)\r\n",
        "\r\n",
        "\r\n",
        "def lr_scheduler(epoch):\r\n",
        "    if epoch <= epochs*0.8:\r\n",
        "        return learning_rate_init\r\n",
        "    else:\r\n",
        "        return learning_rate_init * 0.1\r\n",
        "\r\n",
        "def train_NN(features_oof, features_test, features_train, features_fake):\r\n",
        "    \r\n",
        "    folds = StratifiedKFold(n_splits=n_splits)\r\n",
        "\r\n",
        "    preds_nn_oof = np.zeros(features_oof.shape[0])\r\n",
        "    preds_nn_test = np.zeros(features_test.shape[0])\r\n",
        "    preds_nn_fake = np.zeros(features_fake.shape[0])\r\n",
        "\r\n",
        "    for trn_idx, val_idx in folds.split(features_oof, target_train):\r\n",
        "        features_oof_tr = features_oof[trn_idx, :]\r\n",
        "        target_oof_tr = target_train.values[trn_idx]\r\n",
        "        features_oof_val = features_oof[val_idx, :]\r\n",
        "        target_oof_val = target_train.values[val_idx]\r\n",
        "\r\n",
        "        optimizer = keras.optimizers.Adam(lr = learning_rate_init, decay = 0.00001)\r\n",
        "        model = get_model_3()\r\n",
        "        callbacks = []\r\n",
        "        callbacks.append(keras.callbacks.LearningRateScheduler(lr_scheduler))\r\n",
        "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\r\n",
        "        model.fit(features_oof_tr, target_oof_tr, validation_data=(features_oof_val, target_oof_val), epochs=epochs, verbose=2, batch_size=batch_size, callbacks=callbacks)\r\n",
        "\r\n",
        "        preds_nn_oof += model.predict(features_oof, batch_size=2000)[:,0]\r\n",
        "        preds_nn_test += model.predict(features_test, batch_size=2000)[:,0]\r\n",
        "        preds_nn_fake += model.predict(features_fake, batch_size=2000)[:,0]\r\n",
        "\r\n",
        "        print(roc_auc_score(target_train, preds_nn_oof))\r\n",
        "        if use_experimental:\r\n",
        "            print(roc_auc_score(target_test, preds_nn_test))\r\n",
        "            print(roc_auc_score(target_test, preds_test.mean(axis=1)))\r\n",
        "\r\n",
        "    preds_nn_oof /= n_splits\r\n",
        "    preds_nn_test /= n_splits\r\n",
        "    preds_nn_fake /= n_splits\r\n",
        "    return preds_nn_oof, preds_nn_test, preds_nn_fake\r\n",
        "\r\n",
        "\r\n",
        "features_oof = get_features(preds_oof, X_train)\r\n",
        "features_test = get_features(preds_test, X_test)\r\n",
        "if not use_experimental:\r\n",
        "    del X_test\r\n",
        "features_train = get_features(preds_train, X_train)\r\n",
        "if not use_experimental:\r\n",
        "    del X_train\r\n",
        "features_fake = get_features(preds_fake, X_fake)\r\n",
        "if not use_experimental:\r\n",
        "    del X_fake\r\n",
        "    del preds_oof\r\n",
        "    del preds_fake\r\n",
        "    del preds_train\r\n",
        "    del preds_test\r\n",
        "\r\n",
        "print(get_model_3().summary())\r\n",
        "    \r\n",
        "preds_nn_oof, preds_nn_test, preds_nn_fake = train_NN(features_oof, features_test, features_train, features_fake)\r\n",
        "\r\n",
        "print(roc_auc_score(target_train, preds_nn_oof))\r\n",
        "if use_experimental:\r\n",
        "    print('test AUC: ', roc_auc_score(target_test, preds_nn_test))\r\n",
        "\r\n",
        "\r\n",
        "preds_oof_final = preds_nn_oof\r\n",
        "preds_test_final = preds_nn_test\r\n",
        "preds_fake_final = preds_nn_fake\r\n",
        "\r\n",
        "print('oof  : ', roc_auc_score(target_train, preds_oof_final))\r\n",
        "if use_experimental:\r\n",
        "    print('test : ', roc_auc_score(target_test, preds_test_final))\r\n",
        "    print('train: ', roc_auc_score(target_fake, preds_fake_final))\r\n",
        "\r\n",
        "if not use_experimental:\r\n",
        "    sub = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\r\n",
        "    predictions_all = np.zeros(test_df.shape[0])\r\n",
        "    predictions_all[indices_real] = preds_test_final\r\n",
        "    predictions_all[indices_fake] = preds_fake_final\r\n",
        "    sub[\"target\"] = predictions_all\r\n",
        "    sub.to_csv(\"submission.csv\", index=False)\r\n",
        "    print(sub.head(20))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6gZdPQuzdok"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMRs7_0lzdrE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEly0xelUSRd"
      },
      "source": [
        "#### Programmatic case 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgbypG30USR-"
      },
      "source": [
        "import gc\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import lightgbm as lgb\r\n",
        "import datetime\r\n",
        "\r\n",
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "import json, math, os, sys\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.utils.data\r\n",
        "from multiprocessing import Pool\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "print(device)\r\n",
        "\r\n",
        "import warnings\r\n",
        "warnings.filterwarnings('ignore')\r\n",
        "\r\n",
        "PATH=\"../content/\"#santander-customer-transaction-prediction/\"\r\n",
        "N_SPLITS = 10\r\n",
        "SEED_SKF = 4221\r\n",
        "\r\n",
        "\r\n",
        "def merge_train_test(df_train, df_test):\r\n",
        "    if \"target\" not in df_test.columns.values:\r\n",
        "        df_test[\"target\"] = -1\r\n",
        "    res = pd.concat([df_train, df_test])\r\n",
        "    res.reset_index(inplace=True, drop=True)\r\n",
        "    return res\r\n",
        "\r\n",
        "def split_train_test(df):\r\n",
        "    df_train = df[df[\"target\"] >= 0]\r\n",
        "    df_test = df[df[\"target\"] <= -1]\r\n",
        "    df_train.reset_index(inplace=True, drop=True)\r\n",
        "    df_test.reset_index(inplace=True, drop=True)\r\n",
        "    assert list(df_train[\"ID_code\"].values) == [f\"train_{i}\" for i in range(20000)]\r\n",
        "    assert list(df_test[\"ID_code\"].values) == [f\"test_{i}\" for i in range(20000)]\r\n",
        "    return df_train, df_test\r\n",
        "\r\n",
        "\r\n",
        "class CountEncoder:\r\n",
        "    def fit(self, series):\r\n",
        "        self.counts = series.groupby(series).count()\r\n",
        "    \r\n",
        "    def transform(self, series):\r\n",
        "        return series.map(self.counts).fillna(0).astype(np.int16)\r\n",
        "\r\n",
        "\r\n",
        "# separate into real and fake\r\n",
        "\r\n",
        "df_cnt = pd.DataFrame()\r\n",
        "for v in range(200):\r\n",
        "    sr = test_df[f\"var_{v}\"]\r\n",
        "    enc = CountEncoder()\r\n",
        "    enc.fit(sr)\r\n",
        "    df_cnt[f\"cnt_{v}\"] = enc.transform(sr)\r\n",
        "test_df[\"target\"] = -df_cnt.min(1)  # target==-1 -> real, target==-2 -> fake\r\n",
        "del df_cnt\r\n",
        "\r\n",
        "\r\n",
        "df_merged = merge_train_test(train_df, test_df)\r\n",
        "df_merged.tail()\r\n",
        "\r\n",
        "\r\n",
        "# count encoding\r\n",
        "\r\n",
        "count_enc = [None] * 200\r\n",
        "df_real = df_merged[df_merged[\"target\"]!=-2]\r\n",
        "for v in range(200):\r\n",
        "    enc = CountEncoder()\r\n",
        "    enc.fit(df_real[f\"var_{v}\"])\r\n",
        "    count_enc[v] = enc.transform(df_merged[f\"var_{v}\"])\r\n",
        "    \r\n",
        "for v in range(200):\r\n",
        "    df_merged[f\"cnt_{v}\"] = count_enc[v]\r\n",
        "\r\n",
        "del df_real\r\n",
        "\r\n",
        "\r\n",
        "# normalize\r\n",
        "\r\n",
        "for v in range(200):\r\n",
        "    df_merged[f\"var_{v}_minmax\"] = StandardScaler().fit_transform(df_merged[f\"var_{v}\"].values.reshape(-1, 1))\r\n",
        "    df_merged[f\"cnt_{v}_minmax\"] = MinMaxScaler().fit_transform(df_merged[f\"cnt_{v}\"].values.reshape(-1, 1))\r\n",
        "df_merged.drop(columns=[f\"var_{v}\" for v in range(200)]+[f\"cnt_{v}\" for v in range(200)], inplace=True)\r\n",
        "train_df, test_df = split_train_test(df_merged)\r\n",
        "target = train_df['target']\r\n",
        "gc.collect()\r\n",
        "print(train_df.shape)\r\n",
        "test_df.head()\r\n",
        "\r\n",
        "\r\n",
        "# nn model\r\n",
        "\r\n",
        "class Model(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Model, self).__init__()\r\n",
        "        self.hidden_size = 64\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.conv1 = nn.Conv1d(2, self.hidden_size, kernel_size=1)\r\n",
        "        self.conv2 = nn.Conv1d(self.hidden_size, self.hidden_size*2, kernel_size=1)\r\n",
        "        self.conv3 = nn.Conv1d(self.hidden_size*2, self.hidden_size*4, kernel_size=1)\r\n",
        "        self.conv4 = nn.Conv1d(self.hidden_size*4, self.hidden_size*8, kernel_size=1)\r\n",
        "        self.conv5 = nn.Conv1d(self.hidden_size*8, self.hidden_size*16, kernel_size=1)\r\n",
        "        self.conv6 = nn.Conv1d(self.hidden_size*16, self.hidden_size*32, kernel_size=1)\r\n",
        "        \r\n",
        "        self.fc = nn.Linear(self.hidden_size*32*200, 2)\r\n",
        "        \r\n",
        "    def forward(self, x_):\r\n",
        "        x = self.conv1(x_)\r\n",
        "        x = self.relu(x)\r\n",
        "        \r\n",
        "        x = self.conv2(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        \r\n",
        "        x = self.conv3(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        \r\n",
        "        x = self.conv4(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        \r\n",
        "        x = self.conv5(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        \r\n",
        "        x = self.conv6(x)\r\n",
        "        x = self.relu(x)\r\n",
        "        \r\n",
        "        x = x.view(x.shape[0], -1)\r\n",
        "        x = self.fc(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "# dataset\r\n",
        "\r\n",
        "class TrainData(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, trn_X, trn_y):\r\n",
        "        self.trn_X = trn_X\r\n",
        "        self.trn_y = trn_y\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return self.trn_X.shape[0]\r\n",
        "        \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return self.trn_X[idx], self.trn_y[idx], idx\r\n",
        "    \r\n",
        "    def shuffle(self):\r\n",
        "        trn_X = self.trn_X.to(\"cpu\").numpy()\r\n",
        "        trn_y = self.trn_y.to(\"cpu\").numpy()\r\n",
        "        trn_X_pos = trn_X[trn_y==1].transpose(2,0,1)\r\n",
        "        trn_X_neg = trn_X[trn_y==0].transpose(2,0,1)\r\n",
        "        for c in trn_X_pos:\r\n",
        "            np.random.shuffle(c)\r\n",
        "        for c in trn_X_neg:\r\n",
        "            np.random.shuffle(c)\r\n",
        "        trn_X[trn_y==1] = trn_X_pos.transpose(1,2,0)\r\n",
        "        trn_X[trn_y==0] = trn_X_neg.transpose(1,2,0)\r\n",
        "        self.trn_X = torch.from_numpy(trn_X).to(device)\r\n",
        "    \r\n",
        "class ValidData(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, val_X, val_y):\r\n",
        "        self.val_X = val_X\r\n",
        "        self.val_y = val_y\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return self.val_X.shape[0]\r\n",
        "        \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return self.val_X[idx], self.val_y[idx], idx\r\n",
        "    \r\n",
        "class TestData(torch.utils.data.Dataset):\r\n",
        "    def __init__(self, test_X):\r\n",
        "        self.test_X = test_X\r\n",
        "        \r\n",
        "    def __len__(self):\r\n",
        "        return self.test_X.shape[0]\r\n",
        "        \r\n",
        "    def __getitem__(self, idx):\r\n",
        "        return self.test_X[idx], -1, idx\r\n",
        "\r\n",
        "\r\n",
        "from scipy.special import logit, expit\r\n",
        "\r\n",
        "BATCH_SIZE = 256\r\n",
        "EARLY_STOPPING = 20\r\n",
        "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED_SKF)\r\n",
        "oof = np.zeros(len(train_df))\r\n",
        "fold_oof = np.zeros((N_SPLITS, len(train_df)))\r\n",
        "fold_preds = np.zeros((N_SPLITS, len(test_df)))\r\n",
        "predictions = np.zeros(len(test_df))\r\n",
        "\r\n",
        "loss_func = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "for fold_, (trn_idx, val_idx) in enumerate(skf.split(train_df.values, target.values)):\r\n",
        "    print(\"fold n{}\".format(fold_))\r\n",
        "    \r\n",
        "    features = [f\"var_{v}_minmax\" for v in range(200)] + [f\"cnt_{v}_minmax\" for v in range(200)]\r\n",
        "    \r\n",
        "    trn_X_npy, trn_y_npy = train_df.iloc[trn_idx][features].values.astype(np.float32), target.iloc[trn_idx].values\r\n",
        "    val_X_npy, val_y_npy = train_df.iloc[val_idx][features].values.astype(np.float32), target.iloc[val_idx].values\r\n",
        "    trn_X, trn_y = torch.tensor(trn_X_npy.reshape(-1, 2, 200)).to(device), torch.tensor(trn_y_npy).to(device)       \r\n",
        "    val_X, val_y = torch.tensor(val_X_npy.reshape(-1, 2, 200)).to(device), torch.tensor(val_y_npy).to(device)     \r\n",
        "    test_X = torch.tensor(test_df[features].values.astype(np.float32).reshape(-1, 2, 200)).to(device)\r\n",
        "    trn_dataset = TrainData(trn_X, trn_y)\r\n",
        "    val_dataset = ValidData(val_X, val_y)\r\n",
        "    test_dataset = TestData(test_X)\r\n",
        "    #trn_loader = torch.utils.data.DataLoader(dataset=trn_dataset, batch_size=BATCH_SIZE, shuffle=True)\r\n",
        "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=256) #batch_size=len(val_idx))\r\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=256)\r\n",
        "    filename_net = f\"net_{fold_}.pth\"\r\n",
        "    \r\n",
        "    net = Model().to(device)\r\n",
        "    optimizer = optim.Adam(net.parameters(), lr=0.00002)\r\n",
        "    \r\n",
        "    best_epoch = 0\r\n",
        "    min_auc = 0.5\r\n",
        "    for epoch in range(100):\r\n",
        "        if epoch - EARLY_STOPPING > best_epoch:\r\n",
        "            break\r\n",
        "            \r\n",
        "        # train dataset with shuffling\r\n",
        "        trn_dataset.shuffle()\r\n",
        "        trn_loader = torch.utils.data.DataLoader(dataset=trn_dataset, batch_size=BATCH_SIZE, shuffle=True)\r\n",
        "\r\n",
        "        # train\r\n",
        "        net = net.train()\r\n",
        "        oof_ = np.zeros((len(trn_idx), 2), dtype=np.float32)\r\n",
        "\r\n",
        "        for data, label, idx in trn_loader:\r\n",
        "            optimizer.zero_grad()\r\n",
        "            output = net(data)\r\n",
        "            loss = loss_func(output, label)\r\n",
        "            loss.backward()\r\n",
        "            oof_[idx.numpy()] = output.detach().cpu().numpy()\r\n",
        "            optimizer.step()\r\n",
        "            \r\n",
        "        # eval\r\n",
        "        net = net.eval()\r\n",
        "        with torch.no_grad():\r\n",
        "            # train data\r\n",
        "            loss = loss_func(torch.from_numpy(oof_), torch.from_numpy(trn_y_npy))\r\n",
        "            auc = roc_auc_score(trn_y_npy, oof_[:, 1] - oof_[:, 0])\r\n",
        "            print(f\"epoch {epoch}: train loss: {loss:.5f}, train auc: {auc:.5f}, \", end=\"\")\r\n",
        "\r\n",
        "            # valid data\r\n",
        "            output = np.zeros((len(val_idx), 2), dtype=np.float32)\r\n",
        "            for data, _, idx in val_loader:\r\n",
        "                output[idx.numpy()] = net(data).detach().cpu().numpy()\r\n",
        "            loss = loss_func(torch.from_numpy(output), torch.from_numpy(val_y_npy))\r\n",
        "            auc = roc_auc_score(val_y_npy, output[:, 1] - output[:, 0])\r\n",
        "            print(f\"valid loss: {loss:.5f}, valid auc: {auc:.5f}\")\r\n",
        "\r\n",
        "            if auc > min_auc:\r\n",
        "                torch.save(net.state_dict(), filename_net)\r\n",
        "                min_auc = auc\r\n",
        "                best_epoch = epoch\r\n",
        "\r\n",
        "    net.load_state_dict(torch.load(filename_net))\r\n",
        "    output = np.zeros((len(val_idx), 2), dtype=np.float32)\r\n",
        "    for data, _, idx in val_loader:\r\n",
        "        output[idx.numpy()] = net(data).detach().cpu().numpy()\r\n",
        "    val_auc = roc_auc_score(val_y_npy, output[:, 1] - output[:, 0])\r\n",
        "    print(f\"fold {fold_} auc: {val_auc:.5f}\")\r\n",
        "    oof[val_idx] = expit(output[:, 1] - output[:, 0])\r\n",
        "    fold_oof[fold_, val_idx] = oof[val_idx]\r\n",
        "    \r\n",
        "    output = np.zeros((len(test_dataset), 2), dtype=np.float32)\r\n",
        "    for data, _, idx in test_loader:\r\n",
        "        output[idx.numpy()] = net(data).detach().cpu().numpy()\r\n",
        "    fold_preds[fold_, :] = expit(output[:, 1] - output[:, 0])\r\n",
        "    predictions += fold_preds[fold_] / N_SPLITS\r\n",
        "    \r\n",
        "    break  # due to execution time limitation\r\n",
        "    \r\n",
        "\r\n",
        "np.save(\"oof.npy\", oof)\r\n",
        "np.save(\"fold_oof.npy\", fold_oof)\r\n",
        "np.save(\"fold_preds.npy\", fold_preds)\r\n",
        "np.save(\"predictions.npy\", predictions)\r\n",
        "print(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))\r\n",
        "\r\n",
        "\r\n",
        "sub_df = pd.DataFrame({\"ID_code\":test_df[\"ID_code\"].values})\r\n",
        "sub_df[\"target\"] = predictions\r\n",
        "sub_df.to_csv(\"submission.csv\", index=False)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}